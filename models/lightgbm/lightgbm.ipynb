{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Documents\\Capstone\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import optuna as optuna\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../Dataset/\"\n",
    "x_train = pd.read_csv(path + \"X_train_winsorized.csv\")\n",
    "y_train = pd.read_csv(path + \"Y_train_winsorized.csv\")\n",
    "x_validation = pd.read_csv(path + \"X_validation.csv\")\n",
    "y_validation = pd.read_csv(path + \"Y_validation.csv\")\n",
    "x_test = pd.read_csv(path + \"X_test.csv\")\n",
    "y_test = pd.read_csv(path + \"Y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((910577, 35), (910577, 1), (35646, 35), (35646, 1), (57419, 35), (57419, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_validation.shape, y_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.drop(columns=['StartDate','EventDate','Duration','tic'])\n",
    "x_test = x_test.drop(columns=['StartDate','EventDate','Duration','tic'])\n",
    "x_validation = x_validation.drop(columns=['StartDate','EventDate','Duration','tic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini sample to try:\n",
    "msk = np.random.rand(len(x_train)) < 0.0001\n",
    "x_train_sample=x_train[msk]\n",
    "y_train_sample=y_train[msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:22:17,345] A new study created in memory with name: no-name-d6697f8c-e445-4102-abdb-b3e3d1bd9e08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.103205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:22:46,461] Trial 0 finished with value: 0.9636873435293191 and parameters: {'num_leaves': 84, 'lambda_l1': 1.9743089341965159, 'lambda_l2': 0.034033622149636525, 'feature_fraction': 0.4828511932754119, 'bagging_fraction': 0.8629641279924658, 'bagging_freq': 6, 'min_child_samples': 17}. Best is trial 0 with value: 0.9636873435293191.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:23:45,976] Trial 1 finished with value: 0.9698079711185993 and parameters: {'num_leaves': 217, 'lambda_l1': 0.09942980475624708, 'lambda_l2': 3.76783556651331, 'feature_fraction': 0.8947043803326273, 'bagging_fraction': 0.9878752735723937, 'bagging_freq': 6, 'min_child_samples': 43}. Best is trial 1 with value: 0.9698079711185993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104787 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:24:12,344] Trial 2 finished with value: 0.959497381360892 and parameters: {'num_leaves': 98, 'lambda_l1': 0.8884952103603068, 'lambda_l2': 2.7196243226784613e-08, 'feature_fraction': 0.4740872760125768, 'bagging_fraction': 0.6205814858228127, 'bagging_freq': 5, 'min_child_samples': 20}. Best is trial 1 with value: 0.9698079711185993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.110739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:24:58,154] Trial 3 finished with value: 0.9736193383656773 and parameters: {'num_leaves': 229, 'lambda_l1': 1.5219287153884521e-06, 'lambda_l2': 0.7593809825807439, 'feature_fraction': 0.8630633881420839, 'bagging_fraction': 0.5827415668502326, 'bagging_freq': 3, 'min_child_samples': 87}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:25:30,924] Trial 4 finished with value: 0.9621780241847064 and parameters: {'num_leaves': 255, 'lambda_l1': 8.607260723448295e-05, 'lambda_l2': 2.213362550819861e-07, 'feature_fraction': 0.44630944682662294, 'bagging_fraction': 0.4340509744690155, 'bagging_freq': 3, 'min_child_samples': 33}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:26:01,091] Trial 5 finished with value: 0.9676410982963636 and parameters: {'num_leaves': 76, 'lambda_l1': 6.996256228354022, 'lambda_l2': 5.965259999138856e-05, 'feature_fraction': 0.7328171429696885, 'bagging_fraction': 0.6906511704174683, 'bagging_freq': 1, 'min_child_samples': 93}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:26:24,332] Trial 6 finished with value: 0.9529801277509204 and parameters: {'num_leaves': 54, 'lambda_l1': 3.5691271093795926e-07, 'lambda_l2': 2.530131001159744e-06, 'feature_fraction': 0.5301786476298067, 'bagging_fraction': 0.9648956322648842, 'bagging_freq': 3, 'min_child_samples': 76}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:27:05,205] Trial 7 finished with value: 0.612812474589237 and parameters: {'num_leaves': 252, 'lambda_l1': 9.187318059363433e-07, 'lambda_l2': 3.761924745839708e-08, 'feature_fraction': 0.8030511906236862, 'bagging_fraction': 0.6013162194342064, 'bagging_freq': 3, 'min_child_samples': 43}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:27:27,528] Trial 8 finished with value: 0.9411461427581461 and parameters: {'num_leaves': 76, 'lambda_l1': 0.010337644081153126, 'lambda_l2': 0.00392085341600363, 'feature_fraction': 0.5143931961075968, 'bagging_fraction': 0.5799907203353427, 'bagging_freq': 5, 'min_child_samples': 5}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:28:24,704] Trial 9 finished with value: 0.9683126113347679 and parameters: {'num_leaves': 141, 'lambda_l1': 3.685128983629741e-07, 'lambda_l2': 6.4032842794727785, 'feature_fraction': 0.8736387027133272, 'bagging_fraction': 0.8817867391898959, 'bagging_freq': 4, 'min_child_samples': 29}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:29:04,910] Trial 10 finished with value: 0.9664499406078855 and parameters: {'num_leaves': 154, 'lambda_l1': 2.796648956143541e-08, 'lambda_l2': 0.09989196483393573, 'feature_fraction': 0.9883920971013213, 'bagging_fraction': 0.40813528739073, 'bagging_freq': 1, 'min_child_samples': 67}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:29:58,345] Trial 11 finished with value: 0.972819513697761 and parameters: {'num_leaves': 198, 'lambda_l1': 0.00066659139771853, 'lambda_l2': 4.834702785133855, 'feature_fraction': 0.9115721191210339, 'bagging_fraction': 0.7957866154798259, 'bagging_freq': 7, 'min_child_samples': 58}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.110201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:30:51,731] Trial 12 finished with value: 0.9684940274600847 and parameters: {'num_leaves': 190, 'lambda_l1': 0.00013274305727462973, 'lambda_l2': 0.4858364338772676, 'feature_fraction': 0.9878078117708353, 'bagging_fraction': 0.7798899301016833, 'bagging_freq': 7, 'min_child_samples': 65}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:31:36,055] Trial 13 finished with value: 0.9710693919296755 and parameters: {'num_leaves': 188, 'lambda_l1': 0.002076322273502831, 'lambda_l2': 9.65948336550868, 'feature_fraction': 0.6982560893901705, 'bagging_fraction': 0.7362800813644308, 'bagging_freq': 2, 'min_child_samples': 98}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102576 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:32:20,915] Trial 14 finished with value: 0.9668576761123647 and parameters: {'num_leaves': 209, 'lambda_l1': 1.4267703090999657e-05, 'lambda_l2': 0.003627819162357306, 'feature_fraction': 0.8910727826076857, 'bagging_fraction': 0.5132186980572093, 'bagging_freq': 7, 'min_child_samples': 81}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116108 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:32:35,686] Trial 15 finished with value: 0.9693130465240751 and parameters: {'num_leaves': 10, 'lambda_l1': 8.933809773802657e-06, 'lambda_l2': 0.2676965664858294, 'feature_fraction': 0.7958020851865456, 'bagging_fraction': 0.6675413864260584, 'bagging_freq': 4, 'min_child_samples': 56}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120445 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:33:27,217] Trial 16 finished with value: 0.9675963695370275 and parameters: {'num_leaves': 167, 'lambda_l1': 0.0007241894211022985, 'lambda_l2': 0.9542058537914695, 'feature_fraction': 0.6533345325498138, 'bagging_fraction': 0.7660020123934839, 'bagging_freq': 2, 'min_child_samples': 87}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.123057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:34:18,450] Trial 17 finished with value: 0.9662246100309448 and parameters: {'num_leaves': 231, 'lambda_l1': 0.013724397827469556, 'lambda_l2': 0.018441511428651983, 'feature_fraction': 0.9427418123008463, 'bagging_fraction': 0.5159893995726471, 'bagging_freq': 5, 'min_child_samples': 70}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:35:04,568] Trial 18 finished with value: 0.9683975319961857 and parameters: {'num_leaves': 112, 'lambda_l1': 1.4976003968081104e-08, 'lambda_l2': 0.7255171401480692, 'feature_fraction': 0.8294787631701396, 'bagging_fraction': 0.8103781244625758, 'bagging_freq': 6, 'min_child_samples': 58}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091955 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:35:55,507] Trial 19 finished with value: 0.9665812768089003 and parameters: {'num_leaves': 179, 'lambda_l1': 2.431165838287835e-05, 'lambda_l2': 0.11807636331307814, 'feature_fraction': 0.9380114305444871, 'bagging_fraction': 0.6971805342841858, 'bagging_freq': 4, 'min_child_samples': 46}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:36:45,324] Trial 20 finished with value: 0.9668230098697586 and parameters: {'num_leaves': 231, 'lambda_l1': 0.0002586881109726596, 'lambda_l2': 0.0006249917665293068, 'feature_fraction': 0.8424918326263205, 'bagging_fraction': 0.6420477186034045, 'bagging_freq': 2, 'min_child_samples': 84}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:37:45,936] Trial 21 finished with value: 0.9693788891190923 and parameters: {'num_leaves': 192, 'lambda_l1': 0.0011952042524682864, 'lambda_l2': 3.7820867610782827, 'feature_fraction': 0.732892070026641, 'bagging_fraction': 0.7425182241458317, 'bagging_freq': 2, 'min_child_samples': 94}. Best is trial 3 with value: 0.9736193383656773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.135412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:38:35,959] Trial 22 finished with value: 0.9744519680014295 and parameters: {'num_leaves': 206, 'lambda_l1': 0.003156539372625778, 'lambda_l2': 7.096464592806325, 'feature_fraction': 0.6601604464125688, 'bagging_fraction': 0.7054593817818358, 'bagging_freq': 2, 'min_child_samples': 99}. Best is trial 22 with value: 0.9744519680014295.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:39:23,768] Trial 23 finished with value: 0.9751208054573507 and parameters: {'num_leaves': 214, 'lambda_l1': 0.006565280760681445, 'lambda_l2': 2.094613287916705, 'feature_fraction': 0.6178269855816465, 'bagging_fraction': 0.7009714957558588, 'bagging_freq': 3, 'min_child_samples': 99}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104850 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:40:08,808] Trial 24 finished with value: 0.9717709761885955 and parameters: {'num_leaves': 230, 'lambda_l1': 0.015749334935501128, 'lambda_l2': 0.9232784189959701, 'feature_fraction': 0.6137893731035885, 'bagging_fraction': 0.6595217402170004, 'bagging_freq': 3, 'min_child_samples': 100}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:40:46,432] Trial 25 finished with value: 0.9741282806858194 and parameters: {'num_leaves': 168, 'lambda_l1': 0.10603830282069975, 'lambda_l2': 0.10241433136556377, 'feature_fraction': 0.5879010753282549, 'bagging_fraction': 0.7249159671593745, 'bagging_freq': 1, 'min_child_samples': 89}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105888 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:41:25,867] Trial 26 finished with value: 0.9705429419937208 and parameters: {'num_leaves': 133, 'lambda_l1': 0.29680045604777966, 'lambda_l2': 0.06091487681673318, 'feature_fraction': 0.5840254387306073, 'bagging_fraction': 0.7204387314689478, 'bagging_freq': 1, 'min_child_samples': 92}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164351 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:42:01,390] Trial 27 finished with value: 0.9703682148246116 and parameters: {'num_leaves': 163, 'lambda_l1': 0.05632886040836681, 'lambda_l2': 0.2016722512884105, 'feature_fraction': 0.5729997661797929, 'bagging_fraction': 0.7026710545571795, 'bagging_freq': 1, 'min_child_samples': 75}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:42:43,609] Trial 28 finished with value: 0.9725965678791204 and parameters: {'num_leaves': 173, 'lambda_l1': 0.0031864635798763852, 'lambda_l2': 1.870717276704368, 'feature_fraction': 0.6437471345092199, 'bagging_fraction': 0.7433334912909957, 'bagging_freq': 2, 'min_child_samples': 79}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:43:16,035] Trial 29 finished with value: 0.9704924549155627 and parameters: {'num_leaves': 206, 'lambda_l1': 0.6999124453626755, 'lambda_l2': 0.0288882860100846, 'feature_fraction': 0.4090651604206804, 'bagging_fraction': 0.8320327770565373, 'bagging_freq': 1, 'min_child_samples': 91}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.103243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:44:04,824] Trial 30 finished with value: 0.9698885875821098 and parameters: {'num_leaves': 149, 'lambda_l1': 0.07307983932279553, 'lambda_l2': 0.26615650644294925, 'feature_fraction': 0.5667608997665174, 'bagging_fraction': 0.8618437669673866, 'bagging_freq': 2, 'min_child_samples': 99}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113763 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:44:56,467] Trial 31 finished with value: 0.9707493689987702 and parameters: {'num_leaves': 217, 'lambda_l1': 0.0048423617649409915, 'lambda_l2': 1.4088624685631805, 'feature_fraction': 0.664908583335149, 'bagging_fraction': 0.6681828349550656, 'bagging_freq': 3, 'min_child_samples': 87}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:45:38,187] Trial 32 finished with value: 0.9747165016782301 and parameters: {'num_leaves': 232, 'lambda_l1': 0.04738608561552479, 'lambda_l2': 9.817429377235388, 'feature_fraction': 0.6174042948507226, 'bagging_fraction': 0.6299514071038473, 'bagging_freq': 4, 'min_child_samples': 86}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.114299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:46:19,127] Trial 33 finished with value: 0.9731008570123353 and parameters: {'num_leaves': 244, 'lambda_l1': 0.036867523957167435, 'lambda_l2': 8.971685115751674, 'feature_fraction': 0.6086822215732701, 'bagging_fraction': 0.627135135372462, 'bagging_freq': 4, 'min_child_samples': 95}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.110779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:47:02,510] Trial 34 finished with value: 0.973198224948783 and parameters: {'num_leaves': 214, 'lambda_l1': 0.18774297011456773, 'lambda_l2': 2.1804896718997133, 'feature_fraction': 0.628935936726076, 'bagging_fraction': 0.7054184025277837, 'bagging_freq': 4, 'min_child_samples': 100}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:47:47,008] Trial 35 finished with value: 0.9690070994836242 and parameters: {'num_leaves': 239, 'lambda_l1': 0.02536649783947066, 'lambda_l2': 2.692221222886816, 'feature_fraction': 0.5494397997470524, 'bagging_fraction': 0.7589008484937694, 'bagging_freq': 3, 'min_child_samples': 72}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:48:24,211] Trial 36 finished with value: 0.9659531547385903 and parameters: {'num_leaves': 213, 'lambda_l1': 0.2122328186064799, 'lambda_l2': 0.3717335973268273, 'feature_fraction': 0.6012598271552397, 'bagging_fraction': 0.6733062586457458, 'bagging_freq': 2, 'min_child_samples': 82}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:48:56,440] Trial 37 finished with value: 0.9709085079916739 and parameters: {'num_leaves': 180, 'lambda_l1': 0.004994311563032005, 'lambda_l2': 9.525486223867887, 'feature_fraction': 0.5153271078354915, 'bagging_fraction': 0.6326379372697746, 'bagging_freq': 1, 'min_child_samples': 88}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:49:44,706] Trial 38 finished with value: 0.9658592185275015 and parameters: {'num_leaves': 120, 'lambda_l1': 2.3482316371382566, 'lambda_l2': 2.48421917560544, 'feature_fraction': 0.6803274091908956, 'bagging_fraction': 0.7207558925489902, 'bagging_freq': 5, 'min_child_samples': 91}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:50:31,588] Trial 39 finished with value: 0.9686500255518129 and parameters: {'num_leaves': 222, 'lambda_l1': 0.0905756151302628, 'lambda_l2': 0.6387132875306146, 'feature_fraction': 0.6390202024937269, 'bagging_fraction': 0.5961243997466218, 'bagging_freq': 3, 'min_child_samples': 77}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:51:16,770] Trial 40 finished with value: 0.9681252623961191 and parameters: {'num_leaves': 202, 'lambda_l1': 0.009048648242277083, 'lambda_l2': 0.058627721351953654, 'feature_fraction': 0.5410800216007072, 'bagging_fraction': 0.684740080901364, 'bagging_freq': 5, 'min_child_samples': 85}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:52:03,639] Trial 41 finished with value: 0.9718294318493661 and parameters: {'num_leaves': 247, 'lambda_l1': 0.03610400374838634, 'lambda_l2': 1.7419066999245703, 'feature_fraction': 0.7257861136399417, 'bagging_fraction': 0.5688144732899244, 'bagging_freq': 3, 'min_child_samples': 95}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:52:39,727] Trial 42 finished with value: 0.9747657091299833 and parameters: {'num_leaves': 233, 'lambda_l1': 0.001669572630009752, 'lambda_l2': 3.5456435804726185, 'feature_fraction': 0.48848836260294637, 'bagging_fraction': 0.6189332067230249, 'bagging_freq': 3, 'min_child_samples': 31}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:53:17,826] Trial 43 finished with value: 0.9705785970385491 and parameters: {'num_leaves': 255, 'lambda_l1': 0.0022215996530481636, 'lambda_l2': 4.4111587065573366, 'feature_fraction': 0.5065657633562654, 'bagging_fraction': 0.6465367434953834, 'bagging_freq': 4, 'min_child_samples': 30}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098629 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:53:57,322] Trial 44 finished with value: 0.9734429825811438 and parameters: {'num_leaves': 223, 'lambda_l1': 0.006762870480579347, 'lambda_l2': 3.5341988948304, 'feature_fraction': 0.4538900550930852, 'bagging_fraction': 0.6180358040611817, 'bagging_freq': 2, 'min_child_samples': 41}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:54:38,256] Trial 45 finished with value: 0.970597907764296 and parameters: {'num_leaves': 238, 'lambda_l1': 0.0003785406402204688, 'lambda_l2': 0.5743165530609652, 'feature_fraction': 0.4875103417815766, 'bagging_fraction': 0.6914894665920979, 'bagging_freq': 3, 'min_child_samples': 19}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:55:16,913] Trial 46 finished with value: 0.9709488743882659 and parameters: {'num_leaves': 196, 'lambda_l1': 0.016185136567211048, 'lambda_l2': 0.13127489749129242, 'feature_fraction': 0.5962775517052711, 'bagging_fraction': 0.7254988435189458, 'bagging_freq': 3, 'min_child_samples': 13}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087609 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:55:48,183] Trial 47 finished with value: 0.9716310315917657 and parameters: {'num_leaves': 155, 'lambda_l1': 0.0013106772589188925, 'lambda_l2': 9.988220952694377, 'feature_fraction': 0.5380259138991743, 'bagging_fraction': 0.6129861844060073, 'bagging_freq': 4, 'min_child_samples': 24}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092953 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:56:21,992] Trial 48 finished with value: 0.9727575099819584 and parameters: {'num_leaves': 183, 'lambda_l1': 0.003921474118176127, 'lambda_l2': 1.1597116944229027, 'feature_fraction': 0.5608950899875313, 'bagging_fraction': 0.6526149687870748, 'bagging_freq': 1, 'min_child_samples': 39}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087896 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:57:04,143] Trial 49 finished with value: 0.9690956845297471 and parameters: {'num_leaves': 206, 'lambda_l1': 0.028033908363264057, 'lambda_l2': 3.9394422993413754, 'feature_fraction': 0.6202588372973836, 'bagging_fraction': 0.7755312847677488, 'bagging_freq': 2, 'min_child_samples': 51}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081631 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:57:40,946] Trial 50 finished with value: 0.962742862912809 and parameters: {'num_leaves': 255, 'lambda_l1': 0.0995776285882863, 'lambda_l2': 0.30231652106932033, 'feature_fraction': 0.581783747472984, 'bagging_fraction': 0.5665541027085008, 'bagging_freq': 3, 'min_child_samples': 34}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:58:28,062] Trial 51 finished with value: 0.9704262051666892 and parameters: {'num_leaves': 227, 'lambda_l1': 0.0017347349848589743, 'lambda_l2': 0.9965068553711197, 'feature_fraction': 0.6769181168744702, 'bagging_fraction': 0.5922053390909902, 'bagging_freq': 4, 'min_child_samples': 62}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.106664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:59:15,465] Trial 52 finished with value: 0.9735626276500045 and parameters: {'num_leaves': 238, 'lambda_l1': 0.0004316221215666135, 'lambda_l2': 4.8474687749076875, 'feature_fraction': 0.63889194029725, 'bagging_fraction': 0.55636355973078, 'bagging_freq': 3, 'min_child_samples': 89}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112840 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 22:59:35,320] Trial 53 finished with value: 0.9677998883002479 and parameters: {'num_leaves': 42, 'lambda_l1': 0.00011669276120459193, 'lambda_l2': 0.39454118291973117, 'feature_fraction': 0.7133771083722614, 'bagging_fraction': 0.6743123017469834, 'bagging_freq': 2, 'min_child_samples': 95}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.106747 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:00:16,209] Trial 54 finished with value: 0.9691675762677695 and parameters: {'num_leaves': 196, 'lambda_l1': 0.01064437960404903, 'lambda_l2': 1.766914175165508, 'feature_fraction': 0.7590263986584448, 'bagging_fraction': 0.6126170269125288, 'bagging_freq': 3, 'min_child_samples': 5}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125501 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:00:52,385] Trial 55 finished with value: 0.9696031727289744 and parameters: {'num_leaves': 220, 'lambda_l1': 0.0009382194310732823, 'lambda_l2': 0.15146906564650683, 'feature_fraction': 0.6627514371416596, 'bagging_fraction': 0.5443855904605011, 'bagging_freq': 2, 'min_child_samples': 81}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:01:32,605] Trial 56 finished with value: 0.9712246920433645 and parameters: {'num_leaves': 233, 'lambda_l1': 2.507921347045282e-06, 'lambda_l2': 5.279596958574791, 'feature_fraction': 0.697574599076154, 'bagging_fraction': 0.6479250494304691, 'bagging_freq': 5, 'min_child_samples': 96}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104900 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:02:10,956] Trial 57 finished with value: 0.9734913175603481 and parameters: {'num_leaves': 247, 'lambda_l1': 4.3281616906947924e-05, 'lambda_l2': 0.667871249829943, 'feature_fraction': 0.6204978865079468, 'bagging_fraction': 0.5857664244128037, 'bagging_freq': 4, 'min_child_samples': 85}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:02:39,628] Trial 58 finished with value: 0.9697129879404518 and parameters: {'num_leaves': 101, 'lambda_l1': 0.00023374300124441163, 'lambda_l2': 9.690978388291903, 'feature_fraction': 0.5963534902233109, 'bagging_fraction': 0.6292727238390503, 'bagging_freq': 6, 'min_child_samples': 74}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.131453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:03:18,735] Trial 59 finished with value: 0.9627257042860156 and parameters: {'num_leaves': 170, 'lambda_l1': 0.0019581328491075963, 'lambda_l2': 0.01206216057017464, 'feature_fraction': 0.5587971977623445, 'bagging_fraction': 0.7576077381470399, 'bagging_freq': 1, 'min_child_samples': 11}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:04:02,736] Trial 60 finished with value: 0.9737798151498227 and parameters: {'num_leaves': 208, 'lambda_l1': 5.060774696847719e-08, 'lambda_l2': 1.2732521383988524, 'feature_fraction': 0.6496150049313937, 'bagging_fraction': 0.7145610121661699, 'bagging_freq': 3, 'min_child_samples': 66}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:04:45,803] Trial 61 finished with value: 0.9732250389384498 and parameters: {'num_leaves': 211, 'lambda_l1': 1.4715843129625494e-07, 'lambda_l2': 1.3383011302014247, 'feature_fraction': 0.6571770539871474, 'bagging_fraction': 0.7113689718438789, 'bagging_freq': 3, 'min_child_samples': 67}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:05:27,371] Trial 62 finished with value: 0.968283819740657 and parameters: {'num_leaves': 203, 'lambda_l1': 7.843816212375778e-08, 'lambda_l2': 2.7770450632952213, 'feature_fraction': 0.6874076729854441, 'bagging_fraction': 0.679612974508902, 'bagging_freq': 3, 'min_child_samples': 91}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:06:12,553] Trial 63 finished with value: 0.9674449664673901 and parameters: {'num_leaves': 187, 'lambda_l1': 1.0276103090282016e-08, 'lambda_l2': 0.4348341231816445, 'feature_fraction': 0.6449112919725307, 'bagging_fraction': 0.7345345062249456, 'bagging_freq': 2, 'min_child_samples': 49}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:07:01,208] Trial 64 finished with value: 0.9703904919570245 and parameters: {'num_leaves': 229, 'lambda_l1': 5.473973900170588e-07, 'lambda_l2': 0.9174273700265871, 'feature_fraction': 0.6258406664746194, 'bagging_fraction': 0.7060246969351077, 'bagging_freq': 3, 'min_child_samples': 97}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130628 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:07:50,843] Trial 65 finished with value: 0.9693689429320358 and parameters: {'num_leaves': 217, 'lambda_l1': 2.7305285362382268e-08, 'lambda_l2': 0.20838819968100133, 'feature_fraction': 0.5743812396725491, 'bagging_fraction': 0.6534447991269998, 'bagging_freq': 4, 'min_child_samples': 54}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:08:31,824] Trial 66 finished with value: 0.9713687081787553 and parameters: {'num_leaves': 176, 'lambda_l1': 5.947142132805991e-06, 'lambda_l2': 5.03439614439306, 'feature_fraction': 0.666305551969174, 'bagging_fraction': 0.6862060898629272, 'bagging_freq': 1, 'min_child_samples': 61}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:09:07,303] Trial 67 finished with value: 0.971299375693543 and parameters: {'num_leaves': 137, 'lambda_l1': 0.0007639832778646346, 'lambda_l2': 0.06733054255318636, 'feature_fraction': 0.590425091731927, 'bagging_fraction': 0.7902689029665398, 'bagging_freq': 2, 'min_child_samples': 78}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:09:52,419] Trial 68 finished with value: 0.9735030668573389 and parameters: {'num_leaves': 242, 'lambda_l1': 1.8302663374704283e-07, 'lambda_l2': 2.3060854178975014, 'feature_fraction': 0.6110192307614708, 'bagging_fraction': 0.7435541534388836, 'bagging_freq': 4, 'min_child_samples': 83}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:10:32,235] Trial 69 finished with value: 0.9749004188915201 and parameters: {'num_leaves': 163, 'lambda_l1': 1.5420053351974904e-06, 'lambda_l2': 5.91628661029124, 'feature_fraction': 0.6466453766620391, 'bagging_fraction': 0.6623093281323493, 'bagging_freq': 3, 'min_child_samples': 70}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:11:11,005] Trial 70 finished with value: 0.9717734191117323 and parameters: {'num_leaves': 156, 'lambda_l1': 9.50843593586274e-07, 'lambda_l2': 6.070753483087558, 'feature_fraction': 0.6493161547835569, 'bagging_fraction': 0.6618263116178189, 'bagging_freq': 3, 'min_child_samples': 72}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:11:54,548] Trial 71 finished with value: 0.9693899404380439 and parameters: {'num_leaves': 191, 'lambda_l1': 5.145367891192776e-05, 'lambda_l2': 1.3505318501173407, 'feature_fraction': 0.6768929535460586, 'bagging_fraction': 0.6929443819754771, 'bagging_freq': 3, 'min_child_samples': 68}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:12:29,081] Trial 72 finished with value: 0.9722054093530684 and parameters: {'num_leaves': 162, 'lambda_l1': 5.727680474839586e-08, 'lambda_l2': 2.72587291123853, 'feature_fraction': 0.6327910625044902, 'bagging_fraction': 0.6301387476369641, 'bagging_freq': 2, 'min_child_samples': 88}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098327 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:13:02,138] Trial 73 finished with value: 0.9704988530475874 and parameters: {'num_leaves': 120, 'lambda_l1': 1.7336339374998655e-06, 'lambda_l2': 0.638446546511241, 'feature_fraction': 0.7006934117397182, 'bagging_fraction': 0.7176143650357439, 'bagging_freq': 3, 'min_child_samples': 100}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:13:42,244] Trial 74 finished with value: 0.9740374272110695 and parameters: {'num_leaves': 234, 'lambda_l1': 3.5469593283374806e-07, 'lambda_l2': 6.238698615899208, 'feature_fraction': 0.6072867612185784, 'bagging_fraction': 0.6060531931593693, 'bagging_freq': 2, 'min_child_samples': 90}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:14:20,074] Trial 75 finished with value: 0.9713061809794238 and parameters: {'num_leaves': 146, 'lambda_l1': 0.0037783942701850974, 'lambda_l2': 5.6789132091633014, 'feature_fraction': 0.5958127046842048, 'bagging_fraction': 0.6706306440179818, 'bagging_freq': 1, 'min_child_samples': 63}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:15:02,573] Trial 76 finished with value: 0.9669584176093347 and parameters: {'num_leaves': 222, 'lambda_l1': 0.008631425393421654, 'lambda_l2': 3.015084276307192, 'feature_fraction': 0.5784347258304595, 'bagging_fraction': 0.6039049791652762, 'bagging_freq': 2, 'min_child_samples': 93}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:15:41,994] Trial 77 finished with value: 0.9697207238637179 and parameters: {'num_leaves': 234, 'lambda_l1': 0.016801055723562667, 'lambda_l2': 6.203209888763274, 'feature_fraction': 0.6148578400774077, 'bagging_fraction': 0.6394421146923404, 'bagging_freq': 2, 'min_child_samples': 98}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125116 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:16:17,752] Trial 78 finished with value: 0.9697268893363963 and parameters: {'num_leaves': 200, 'lambda_l1': 4.3602500819432325e-07, 'lambda_l2': 1.6662481071599446, 'feature_fraction': 0.5536159130154643, 'bagging_fraction': 0.7013248036895501, 'bagging_freq': 4, 'min_child_samples': 79}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.103502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:16:56,440] Trial 79 finished with value: 0.9743844386261512 and parameters: {'num_leaves': 249, 'lambda_l1': 2.391914783409329e-07, 'lambda_l2': 8.39382623442167, 'feature_fraction': 0.6297841504126765, 'bagging_fraction': 0.6564321891815943, 'bagging_freq': 1, 'min_child_samples': 89}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086892 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:17:36,819] Trial 80 finished with value: 0.9728891370071562 and parameters: {'num_leaves': 249, 'lambda_l1': 2.247503418777837e-07, 'lambda_l2': 9.600273475976257, 'feature_fraction': 0.5299001048185839, 'bagging_fraction': 0.6614692160475661, 'bagging_freq': 1, 'min_child_samples': 90}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:18:17,488] Trial 81 finished with value: 0.9717389273636359 and parameters: {'num_leaves': 212, 'lambda_l1': 9.45633681243246e-08, 'lambda_l2': 3.5427776672449625, 'feature_fraction': 0.6344867204623065, 'bagging_fraction': 0.6815940466645898, 'bagging_freq': 1, 'min_child_samples': 86}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.100109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:18:59,853] Trial 82 finished with value: 0.9749417740903339 and parameters: {'num_leaves': 226, 'lambda_l1': 2.3176587113446647e-07, 'lambda_l2': 2.0176968974363185, 'feature_fraction': 0.6053524987435841, 'bagging_fraction': 0.6397689129838077, 'bagging_freq': 1, 'min_child_samples': 94}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118905 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:19:42,373] Trial 83 finished with value: 0.9736595884325961 and parameters: {'num_leaves': 240, 'lambda_l1': 2.9598641917692964e-07, 'lambda_l2': 6.526230183495981, 'feature_fraction': 0.6136348854547533, 'bagging_fraction': 0.6421062302999009, 'bagging_freq': 1, 'min_child_samples': 93}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113342 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:20:23,397] Trial 84 finished with value: 0.9715280798310056 and parameters: {'num_leaves': 227, 'lambda_l1': 3.378832337814435e-07, 'lambda_l2': 2.3301033138954375, 'feature_fraction': 0.6007162021803892, 'bagging_fraction': 0.6181383929492167, 'bagging_freq': 1, 'min_child_samples': 98}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:21:00,309] Trial 85 finished with value: 0.9734211126025868 and parameters: {'num_leaves': 248, 'lambda_l1': 7.489045768015926e-07, 'lambda_l2': 3.595378569391117, 'feature_fraction': 0.5738205029248978, 'bagging_fraction': 0.6033214699339144, 'bagging_freq': 1, 'min_child_samples': 93}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111801 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:21:38,068] Trial 86 finished with value: 0.9727178997282423 and parameters: {'num_leaves': 256, 'lambda_l1': 0.005935244944288994, 'lambda_l2': 0.8457849665885293, 'feature_fraction': 0.5872127941325147, 'bagging_fraction': 0.6406434517998612, 'bagging_freq': 1, 'min_child_samples': 89}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095249 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:22:16,854] Trial 87 finished with value: 0.9709464896299657 and parameters: {'num_leaves': 235, 'lambda_l1': 1.2085794551007315e-07, 'lambda_l2': 0.43188502191373074, 'feature_fraction': 0.6303067035318591, 'bagging_fraction': 0.6601075463238535, 'bagging_freq': 1, 'min_child_samples': 84}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.100230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:22:57,549] Trial 88 finished with value: 0.9721379963074634 and parameters: {'num_leaves': 225, 'lambda_l1': 0.003204216308917732, 'lambda_l2': 7.282375866548074, 'feature_fraction': 0.6651169441521185, 'bagging_fraction': 0.6223837283356162, 'bagging_freq': 2, 'min_child_samples': 95}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:23:38,832] Trial 89 finished with value: 0.973159138178596 and parameters: {'num_leaves': 218, 'lambda_l1': 0.01951495589650295, 'lambda_l2': 1.9795782141983547, 'feature_fraction': 0.6074060268161098, 'bagging_fraction': 0.6715270397058295, 'bagging_freq': 2, 'min_child_samples': 80}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:24:16,721] Trial 90 finished with value: 0.9702991149987454 and parameters: {'num_leaves': 242, 'lambda_l1': 1.8436110582484304e-07, 'lambda_l2': 3.857984390652513, 'feature_fraction': 0.5691853649545978, 'bagging_fraction': 0.5801566826797875, 'bagging_freq': 1, 'min_child_samples': 87}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.132199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:24:58,970] Trial 91 finished with value: 0.9720687801519242 and parameters: {'num_leaves': 208, 'lambda_l1': 4.1174625298459536e-08, 'lambda_l2': 1.1303612957449318, 'feature_fraction': 0.6482355285194897, 'bagging_fraction': 0.6944090503488478, 'bagging_freq': 3, 'min_child_samples': 59}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.110069 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:25:42,854] Trial 92 finished with value: 0.9668835012998096 and parameters: {'num_leaves': 184, 'lambda_l1': 2.5940957011882543e-07, 'lambda_l2': 1.6435123501671762, 'feature_fraction': 0.6534120611643504, 'bagging_fraction': 0.7286759053486885, 'bagging_freq': 3, 'min_child_samples': 91}. Best is trial 23 with value: 0.9751208054573507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113549 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:26:45,355] Trial 93 finished with value: 0.975419656387738 and parameters: {'num_leaves': 196, 'lambda_l1': 7.199205108584692e-08, 'lambda_l2': 9.29309691735033, 'feature_fraction': 0.6331214357865923, 'bagging_fraction': 0.71437982476929, 'bagging_freq': 2, 'min_child_samples': 96}. Best is trial 93 with value: 0.975419656387738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:27:32,108] Trial 94 finished with value: 0.972175454462226 and parameters: {'num_leaves': 197, 'lambda_l1': 1.0577065530863303e-07, 'lambda_l2': 9.293514500207506, 'feature_fraction': 0.6270691758159275, 'bagging_fraction': 0.651914105497418, 'bagging_freq': 2, 'min_child_samples': 97}. Best is trial 93 with value: 0.975419656387738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:27:43,397] Trial 95 finished with value: 0.9698483375151912 and parameters: {'num_leaves': 4, 'lambda_l1': 5.134062169054706e-07, 'lambda_l2': 3.5997893414242874, 'feature_fraction': 0.6027937902304679, 'bagging_fraction': 0.683554700356818, 'bagging_freq': 2, 'min_child_samples': 100}. Best is trial 93 with value: 0.975419656387738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101854 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:28:21,312] Trial 96 finished with value: 0.9730069208012462 and parameters: {'num_leaves': 232, 'lambda_l1': 0.05074427034155327, 'lambda_l2': 5.772981065868785, 'feature_fraction': 0.586305031544175, 'bagging_fraction': 0.7041425728746218, 'bagging_freq': 1, 'min_child_samples': 94}. Best is trial 93 with value: 0.975419656387738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:29:00,822] Trial 97 finished with value: 0.9701221775658458 and parameters: {'num_leaves': 215, 'lambda_l1': 3.080077649646174e-08, 'lambda_l2': 6.232274797285333, 'feature_fraction': 0.6410114704576053, 'bagging_fraction': 0.6322834767272021, 'bagging_freq': 2, 'min_child_samples': 97}. Best is trial 93 with value: 0.975419656387738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:29:36,261] Trial 98 finished with value: 0.9654380469457703 and parameters: {'num_leaves': 222, 'lambda_l1': 1.350054678777226e-07, 'lambda_l2': 2.243889568755276, 'feature_fraction': 0.6181143284085292, 'bagging_fraction': 0.5913880049029887, 'bagging_freq': 1, 'min_child_samples': 82}. Best is trial 93 with value: 0.975419656387738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-23 23:30:14,449] Trial 99 finished with value: 0.972972545382823 and parameters: {'num_leaves': 168, 'lambda_l1': 0.010667888792571429, 'lambda_l2': 0.7839343759377698, 'feature_fraction': 0.5601299284752055, 'bagging_fraction': 0.7426705072715883, 'bagging_freq': 2, 'min_child_samples': 90}. Best is trial 93 with value: 0.975419656387738.\n"
     ]
    }
   ],
   "source": [
    "# WARNING: 2 changes to change evaluation metric\n",
    "optimisation_metric = \"auc\" #\"accuracy\" \"recall\" \"precision\" \"f1\" \"auc\"\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'objective': 'binary',\n",
    "        'metric': 'precision', #\"accuracy\" \"recall\" \"precision\" \"f1\" \"auc\"\n",
    "        'boosting_type': 'gbdt',\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "    \n",
    "    # full data\n",
    "    gbm = lgb.train(param, lgb.Dataset(x_train, y_train), num_boost_round=500)\n",
    "    \n",
    "    preds = gbm.predict(x_validation)\n",
    "    y_pred_binary = np.round(preds)\n",
    "    \n",
    "    auc = roc_auc_score(y_validation, preds)\n",
    "    accuracy = accuracy_score(y_validation, y_pred_binary)\n",
    "    recall = recall_score(y_validation, y_pred_binary)\n",
    "    precision = precision_score(y_validation, y_pred_binary)\n",
    "    f1 = f1_score(y_validation, y_pred_binary)\n",
    "    #choose the metric you want to optimized\n",
    "    \n",
    "    match optimisation_metric:\n",
    "        case \"auc\":\n",
    "            return auc\n",
    "        case \"recall\":\n",
    "            return recall\n",
    "        case \"precision\":\n",
    "            return precision\n",
    "        case \"f1\":\n",
    "            return f1\n",
    "        case _:\n",
    "            return auc\n",
    "    return auc\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Documents\\Capstone\\.venv\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6171, number of negative: 904406\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109766 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8655\n",
      "[LightGBM] [Info] Number of data points in the train set: 910577, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006777 -> initscore=-4.987417\n",
      "[LightGBM] [Info] Start training from score -4.987417\n"
     ]
    }
   ],
   "source": [
    "# WARNING: 1 changement to chose evaluation metric, should allign with previous cell\n",
    "best_params['objective'] = 'binary'\n",
    "best_params['metric'] = 'auc' #\"accuracy\" \"recall\" \"precision\" \"f1\" \"auc\"\n",
    "best_params['num_boost_round'] = 500\n",
    "\n",
    "gbm = lgb.train(best_params, lgb.Dataset(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(x_test)\n",
    "y_pred_binary = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9859614105661384\n",
      "0.853315673848335\n",
      "0.9005305026447933\n",
      "0.6655773382210621\n",
      "0.6661219603583606\n",
      "0.9927532872598344\n",
      "0.5718835835162356\n",
      "0.7671947084413563\n",
      "0.5985441135232504\n",
      "0.9986396654222602\n",
      "0.9847260677472983\n",
      "0.9847260677472983\n",
      "0.9053912791077047\n",
      "0.8664800424880402\n",
      "0.6149118815059738\n",
      "0.5527890387470105\n",
      "0.8661772196553814\n",
      "0.9846477984372463\n",
      "0.6258186573496543\n",
      "0.9448081696224029\n",
      "0.5760756175758571\n",
      "0.758999682860306\n",
      "0.8728082844042839\n",
      "0.9755320136122735\n",
      "0.9846477984372463\n",
      "0.5356823438732697\n",
      "0.7497580975479636\n",
      "0.9306153364384838\n",
      "0.5718835835162356\n",
      "0.9580763568264696\n",
      "0.5718835835162356\n",
      "0.9488964763678299\n",
      "0.78917252943417\n",
      "0.6524661152530559\n",
      "0.827074669247045\n",
      "0.7553554969882975\n",
      "0.8525584457919788\n",
      "0.7867053724465456\n",
      "0.8262219320178741\n",
      "0.8664800424880402\n",
      "0.758999682860306\n",
      "0.6064458788863022\n",
      "0.9759869457162625\n",
      "0.8063722857271868\n",
      "0.5508599395321008\n",
      "0.5485688105646499\n",
      "0.9369819056131319\n",
      "0.9624495773591969\n",
      "0.8804933865578926\n",
      "0.9748036583199928\n",
      "0.8963649778772644\n",
      "0.8743968699691264\n",
      "0.758999682860306\n",
      "0.8063722857271868\n",
      "0.5786049571431862\n",
      "0.9755320136122735\n",
      "0.5913836235735208\n",
      "0.518398691919065\n",
      "0.6974975514899442\n",
      "0.7867053724465456\n",
      "0.9624495773591969\n",
      "0.9053996208250503\n",
      "0.8759625333580106\n",
      "0.827074669247045\n",
      "0.8963649778772644\n",
      "0.7144148196100546\n",
      "0.6974975514899442\n",
      "0.9488964763678299\n",
      "0.7342191805337489\n",
      "0.9846477984372463\n",
      "0.716677435426253\n",
      "0.5333278544509066\n",
      "0.749845144892916\n",
      "0.9005305026447933\n",
      "0.7795078116040273\n",
      "0.7380852188831798\n",
      "0.9053912791077047\n",
      "0.7717450236672436\n",
      "0.7671947084413563\n",
      "0.6978115419788264\n",
      "0.8778503474253465\n",
      "0.982793143930547\n",
      "0.8804933865578926\n",
      "0.9822908270318943\n",
      "0.9847260677472983\n",
      "0.587394287882709\n",
      "0.8041083584669461\n",
      "0.8963649778772644\n",
      "0.8804933865578926\n",
      "0.8778503474253465\n",
      "0.9369819056131319\n",
      "0.8210878766550671\n",
      "0.8788697567257429\n",
      "0.7380852188831798\n",
      "0.758180204025156\n",
      "0.7867053724465456\n",
      "0.6655773382210621\n",
      "0.6968926077056727\n",
      "0.5831928734224373\n",
      "0.8041083584669461\n",
      "0.6845480708081729\n",
      "0.9369819056131319\n",
      "0.5284252988077487\n",
      "0.7717450236672436\n",
      "0.86143825520488\n",
      "0.9193311037095688\n",
      "0.7795078116040273\n",
      "0.9053996208250503\n",
      "0.8003779727813986\n",
      "0.9365205127068137\n",
      "0.7973514003955283\n",
      "0.9624495773591969\n",
      "0.8003779727813986\n",
      "0.5913836235735208\n",
      "0.7786617274137544\n",
      "0.9005305026447933\n",
      "0.5613018834857115\n",
      "0.6156289640409341\n",
      "0.6354369308546488\n",
      "0.7144148196100546\n",
      "0.9560615138042002\n",
      "0.8743968699691264\n",
      "0.7533356977024911\n",
      "0.9887621536251933\n",
      "0.9667754260660005\n",
      "0.9986396654222602\n",
      "0.518398691919065\n",
      "0.5331586263514015\n",
      "0.587394287882709\n",
      "0.758999682860306\n",
      "0.5913836235735208\n",
      "0.7342191805337489\n",
      "0.7671947084413563\n",
      "0.758180204025156\n",
      "0.86143825520488\n",
      "0.9448081696224029\n",
      "0.9560615138042002\n",
      "0.9755320136122735\n",
      "0.8759625333580106\n",
      "0.587394287882709\n",
      "0.8648448464977033\n",
      "0.8062532578194759\n",
      "0.5331586263514015\n",
      "0.8743968699691264\n",
      "0.8210878766550671\n",
      "0.8041083584669461\n",
      "0.9927532872598344\n",
      "0.5718835835162356\n",
      "0.827074669247045\n",
      "0.7533356977024911\n",
      "0.7144148196100546\n",
      "0.685339359904301\n",
      "0.7969455522999742\n",
      "0.6968926077056727\n",
      "0.7355640976447797\n",
      "0.9448081696224029\n",
      "0.685339359904301\n",
      "0.8228578715091422\n",
      "0.758999682860306\n",
      "0.8262219320178741\n",
      "0.9986396654222602\n",
      "0.8228578715091422\n",
      "0.9927532872598344\n",
      "0.6258186573496543\n",
      "0.518398691919065\n",
      "0.6161785273744295\n",
      "0.8664800424880402\n",
      "0.827074669247045\n",
      "0.7969455522999742\n",
      "0.9053912791077047\n",
      "0.8743968699691264\n",
      "0.9560615138042002\n",
      "0.685339359904301\n",
      "0.982793143930547\n",
      "0.827074669247045\n",
      "0.9900664420636133\n",
      "0.7380852188831798\n",
      "0.716677435426253\n",
      "0.7984446965660051\n",
      "0.7717450236672436\n",
      "0.9532065203045246\n",
      "0.6845480708081729\n",
      "0.827074669247045\n",
      "0.7342191805337489\n",
      "0.8228578715091422\n",
      "0.5786049571431862\n",
      "0.8387267318569298\n",
      "0.7671947084413563\n",
      "0.7973514003955283\n",
      "0.8689328569540768\n",
      "0.5662240591225758\n",
      "0.7818477184172401\n",
      "0.86143825520488\n",
      "0.9053996208250503\n",
      "0.5760756175758571\n",
      "0.8759625333580106\n",
      "0.5249521929199126\n",
      "0.5333278544509066\n",
      "0.7818477184172401\n",
      "0.9193311037095688\n",
      "0.915229625436164\n",
      "0.6655773382210621\n",
      "0.8778503474253465\n",
      "0.9759869457162625\n",
      "0.9878497645109493\n",
      "0.7533356977024911\n",
      "0.716677435426253\n",
      "0.6845480708081729\n",
      "0.9488964763678299\n",
      "0.6132362748809582\n",
      "0.9759869457162625\n",
      "0.9667754260660005\n",
      "0.7717450236672436\n",
      "0.6968926077056727\n",
      "0.8062532578194759\n",
      "0.7717450236672436\n",
      "0.8062532578194759\n",
      "0.7867053724465456\n",
      "0.5760756175758571\n",
      "0.7973514003955283\n",
      "0.9297392136491013\n",
      "0.7078810813892005\n",
      "0.7818477184172401\n",
      "0.8210878766550671\n",
      "0.5333278544509066\n",
      "0.758180204025156\n",
      "0.6974975514899442\n",
      "0.7342191805337489\n",
      "0.8262219320178741\n",
      "0.8003779727813986\n",
      "0.982793143930547\n",
      "0.9488964763678299\n",
      "0.915229625436164\n",
      "0.5760756175758571\n",
      "0.758999682860306\n",
      "0.6851765113145943\n",
      "0.9667754260660005\n",
      "0.7717450236672436\n",
      "0.6325936406890297\n",
      "0.8713033931757481\n",
      "0.5485385672276073\n",
      "0.8713033931757481\n",
      "0.5925411767051374\n",
      "0.6256094349423011\n",
      "0.5485385672276073\n",
      "0.5485385672276073\n",
      "0.5034847939975229\n",
      "0.5034847939975229\n",
      "0.5034847939975229\n",
      "0.5109756226676477\n",
      "0.8713033931757481\n",
      "0.6060136161339421\n",
      "0.5957322923938506\n",
      "0.5957322923938506\n",
      "0.5957322923938506\n",
      "0.5702224808357231\n"
     ]
    }
   ],
   "source": [
    "for pred in y_pred:\n",
    "    if pred > 0.5:\n",
    "        print(str(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.995034505975425\n",
      "Recall: 0.12179487179487179\n",
      "Precision: 0.3220338983050847\n",
      "F1 Score: 0.17674418604651163\n",
      "AUC: 0.9515515385337872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1e9043da5c0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred_binary)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Recall:', recall)\n",
    "print('Precision:', precision)\n",
    "print('F1 Score:', f1)\n",
    "print('AUC:', auc)\n",
    "\n",
    "ConfusionMatrixDisplay(confusion).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = ['Accuracy', 'Recall', 'Precision', 'F1 Score', 'AUC']\n",
    "values = [accuracy, recall, precision, f1, auc]\n",
    "colors = sns.color_palette('pastel', len(metrics))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(metrics, values, color=colors)\n",
    "plt.xlabel('Score')\n",
    "plt.title('Model Performance')\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     feature  importance\n",
      "1            DTDmedianNonFin        5268\n",
      "19         price_to_earnings        2736\n",
      "3                        sic        2503\n",
      "20           retention_ratio        2122\n",
      "7                 cash_ratio        1906\n",
      "13    cashflow_to_debt_ratio        1628\n",
      "14         net_profit_margin        1627\n",
      "18  working_capital_turnover        1473\n",
      "4                        atq        1451\n",
      "15            asset_turnover        1420\n",
      "8        net_working_capital        1395\n",
      "16      receivables_turnover        1313\n",
      "5              current_ratio        1308\n",
      "6                quick_ratio        1305\n",
      "10      debt_to_equity_ratio        1260\n",
      "17     day_sales_outstanding        1218\n",
      "12  financial_leverage_ratio        1204\n",
      "11              equity_ratio        1143\n",
      "9                 debt_ratio        1058\n",
      "0               DTDmedianFin         662\n",
      "2                dummy297fin           0\n"
     ]
    }
   ],
   "source": [
    "importance = pd.DataFrame({'feature': X_train.columns, 'importance': gbm.feature_importance()})\n",
    "importance = importance.sort_values('importance', ascending=False)\n",
    "print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = gbm.feature_importance()\n",
    "features = X_train.columns\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importance})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.sort_values(by='Importance', ascending=False))\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048622 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5081\n",
      "[LightGBM] [Info] Number of data points in the train set: 463380, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score -0.007880\n",
      "[LightGBM] [Info] Start training from score -4.850693\n",
      "[LightGBM] [Info] Start training from score -10.561396\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "model = LGBMClassifier()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "pred = model.predict(X_test_scaled)\n",
    "y_pred_binary = np.round(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.9921\n",
      "Testing accuracy 0.9919\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy {:.4f}'.format(model.score(X_train_scaled,y_train)))\n",
    "print('Testing accuracy {:.4f}'.format(model.score(X_test_scaled,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00    115004\n",
      "         1.0       0.28      0.07      0.11       839\n",
      "         2.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.99    115845\n",
      "   macro avg       0.42      0.36      0.37    115845\n",
      "weighted avg       0.99      0.99      0.99    115845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,model.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Testing LightGBM model with filled_table</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "import pandas_gbq as gbq\n",
    "credentials_path = 'data_cleaning/token.json'\n",
    "\n",
    "# Authenticate with your credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    credentials_path, scopes=['https://www.googleapis.com/auth/bigquery'])\n",
    "\n",
    "# Set the credentials for pandas_gbq\n",
    "gbq.context.credentials = credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'capstone-398012'\n",
    "dataset_id = 'capstone'\n",
    "table_id = \"filled_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/google/cloud/bigquery/table.py:2155: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompNo</th>\n",
       "      <th>yyyy</th>\n",
       "      <th>mm</th>\n",
       "      <th>StkIndx</th>\n",
       "      <th>STInt</th>\n",
       "      <th>dtdlevel</th>\n",
       "      <th>dtdtrend</th>\n",
       "      <th>liqnonfinlevel</th>\n",
       "      <th>liqnonfintrend</th>\n",
       "      <th>ni2talevel</th>\n",
       "      <th>...</th>\n",
       "      <th>nan_count</th>\n",
       "      <th>liqnonfinlevel_notNA</th>\n",
       "      <th>liqnonfintrend_notNA</th>\n",
       "      <th>dtdlevel_notNA</th>\n",
       "      <th>dtdtrend_notNA</th>\n",
       "      <th>DTDmedianNonFin_notNA</th>\n",
       "      <th>Sector_Number_notNA</th>\n",
       "      <th>DTDmedianFin_notNA</th>\n",
       "      <th>sigma_notNA</th>\n",
       "      <th>StkIndx_notNA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26995</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>3.176331</td>\n",
       "      <td>-0.148469</td>\n",
       "      <td>0.280325</td>\n",
       "      <td>-0.146216</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26996</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26996</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27000</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27000</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12335</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12337</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12338</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12339 rows  34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CompNo  yyyy mm   StkIndx     STInt  dtdlevel  dtdtrend  \\\n",
       "0       26995  1990  1  0.106263  0.020305  3.176331 -0.148469   \n",
       "1       26996  1990  1  0.106263  0.020305  4.401022  0.054801   \n",
       "2       26996  1990  1  0.106263  0.020305  4.401022  0.054801   \n",
       "3       27000  1990  1  0.106263  0.020305  2.908823  0.056226   \n",
       "4       27000  1990  1  0.106263  0.020305  2.908823  0.056226   \n",
       "...       ...   ... ..       ...       ...       ...       ...   \n",
       "12334   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12335   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12336   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12337   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12338   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "\n",
       "       liqnonfinlevel  liqnonfintrend  ni2talevel  ...  nan_count  \\\n",
       "0            0.280325       -0.146216    0.002395  ...          3   \n",
       "1            0.951410        0.033574    0.002635  ...          3   \n",
       "2            0.951410        0.033574    0.002635  ...          3   \n",
       "3           -0.312616       -0.149733    0.004073  ...          3   \n",
       "4           -0.312616       -0.149733    0.004073  ...          3   \n",
       "...               ...             ...         ...  ...        ...   \n",
       "12334        0.673112       -0.089241    0.006030  ...          0   \n",
       "12335        0.673112       -0.089241    0.006030  ...          0   \n",
       "12336        0.673112       -0.089241    0.006030  ...          0   \n",
       "12337        0.673112       -0.089241    0.006030  ...          0   \n",
       "12338        0.673112       -0.089241    0.006030  ...          0   \n",
       "\n",
       "       liqnonfinlevel_notNA  liqnonfintrend_notNA  dtdlevel_notNA  \\\n",
       "0                         1                     1               0   \n",
       "1                         1                     1               0   \n",
       "2                         1                     1               0   \n",
       "3                         1                     1               0   \n",
       "4                         1                     1               0   \n",
       "...                     ...                   ...             ...   \n",
       "12334                     1                     1               1   \n",
       "12335                     1                     1               1   \n",
       "12336                     1                     1               1   \n",
       "12337                     1                     1               1   \n",
       "12338                     1                     1               1   \n",
       "\n",
       "       dtdtrend_notNA  DTDmedianNonFin_notNA  Sector_Number_notNA  \\\n",
       "0                   0                      0                    1   \n",
       "1                   0                      0                    1   \n",
       "2                   0                      0                    1   \n",
       "3                   0                      0                    1   \n",
       "4                   0                      0                    1   \n",
       "...               ...                    ...                  ...   \n",
       "12334               1                      1                    1   \n",
       "12335               1                      1                    1   \n",
       "12336               1                      1                    1   \n",
       "12337               1                      1                    1   \n",
       "12338               1                      1                    1   \n",
       "\n",
       "       DTDmedianFin_notNA  sigma_notNA  StkIndx_notNA  \n",
       "0                       1            1              1  \n",
       "1                       1            1              1  \n",
       "2                       1            1              1  \n",
       "3                       1            1              1  \n",
       "4                       1            1              1  \n",
       "...                   ...          ...            ...  \n",
       "12334                   1            1              1  \n",
       "12335                   1            1              1  \n",
       "12336                   1            1              1  \n",
       "12337                   1            1              1  \n",
       "12338                   1            1              1  \n",
       "\n",
       "[12339 rows x 34 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas_gbq import read_gbq\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authenticate and read data from BigQuery into a DataFrame\n",
    "df = read_gbq(query, project_id=project_id, dialect='standard')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompNo</th>\n",
       "      <th>yyyy</th>\n",
       "      <th>mm</th>\n",
       "      <th>StkIndx</th>\n",
       "      <th>STInt</th>\n",
       "      <th>dtdlevel</th>\n",
       "      <th>dtdtrend</th>\n",
       "      <th>liqnonfinlevel</th>\n",
       "      <th>liqnonfintrend</th>\n",
       "      <th>ni2talevel</th>\n",
       "      <th>...</th>\n",
       "      <th>nan_count</th>\n",
       "      <th>liqnonfinlevel_notNA</th>\n",
       "      <th>liqnonfintrend_notNA</th>\n",
       "      <th>dtdlevel_notNA</th>\n",
       "      <th>dtdtrend_notNA</th>\n",
       "      <th>DTDmedianNonFin_notNA</th>\n",
       "      <th>Sector_Number_notNA</th>\n",
       "      <th>DTDmedianFin_notNA</th>\n",
       "      <th>sigma_notNA</th>\n",
       "      <th>StkIndx_notNA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26995</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>3.176331</td>\n",
       "      <td>-0.148469</td>\n",
       "      <td>0.280325</td>\n",
       "      <td>-0.146216</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26996</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26996</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27000</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27000</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12335</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12337</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12338</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12178 rows  34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CompNo  yyyy mm   StkIndx     STInt  dtdlevel  dtdtrend  \\\n",
       "0       26995  1990  1  0.106263  0.020305  3.176331 -0.148469   \n",
       "1       26996  1990  1  0.106263  0.020305  4.401022  0.054801   \n",
       "2       26996  1990  1  0.106263  0.020305  4.401022  0.054801   \n",
       "3       27000  1990  1  0.106263  0.020305  2.908823  0.056226   \n",
       "4       27000  1990  1  0.106263  0.020305  2.908823  0.056226   \n",
       "...       ...   ... ..       ...       ...       ...       ...   \n",
       "12334   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12335   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12336   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12337   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12338   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "\n",
       "       liqnonfinlevel  liqnonfintrend  ni2talevel  ...  nan_count  \\\n",
       "0            0.280325       -0.146216    0.002395  ...          3   \n",
       "1            0.951410        0.033574    0.002635  ...          3   \n",
       "2            0.951410        0.033574    0.002635  ...          3   \n",
       "3           -0.312616       -0.149733    0.004073  ...          3   \n",
       "4           -0.312616       -0.149733    0.004073  ...          3   \n",
       "...               ...             ...         ...  ...        ...   \n",
       "12334        0.673112       -0.089241    0.006030  ...          0   \n",
       "12335        0.673112       -0.089241    0.006030  ...          0   \n",
       "12336        0.673112       -0.089241    0.006030  ...          0   \n",
       "12337        0.673112       -0.089241    0.006030  ...          0   \n",
       "12338        0.673112       -0.089241    0.006030  ...          0   \n",
       "\n",
       "       liqnonfinlevel_notNA  liqnonfintrend_notNA  dtdlevel_notNA  \\\n",
       "0                         1                     1               0   \n",
       "1                         1                     1               0   \n",
       "2                         1                     1               0   \n",
       "3                         1                     1               0   \n",
       "4                         1                     1               0   \n",
       "...                     ...                   ...             ...   \n",
       "12334                     1                     1               1   \n",
       "12335                     1                     1               1   \n",
       "12336                     1                     1               1   \n",
       "12337                     1                     1               1   \n",
       "12338                     1                     1               1   \n",
       "\n",
       "       dtdtrend_notNA  DTDmedianNonFin_notNA  Sector_Number_notNA  \\\n",
       "0                   0                      0                    1   \n",
       "1                   0                      0                    1   \n",
       "2                   0                      0                    1   \n",
       "3                   0                      0                    1   \n",
       "4                   0                      0                    1   \n",
       "...               ...                    ...                  ...   \n",
       "12334               1                      1                    1   \n",
       "12335               1                      1                    1   \n",
       "12336               1                      1                    1   \n",
       "12337               1                      1                    1   \n",
       "12338               1                      1                    1   \n",
       "\n",
       "       DTDmedianFin_notNA  sigma_notNA  StkIndx_notNA  \n",
       "0                       1            1              1  \n",
       "1                       1            1              1  \n",
       "2                       1            1              1  \n",
       "3                       1            1              1  \n",
       "4                       1            1              1  \n",
       "...                   ...          ...            ...  \n",
       "12334                   1            1              1  \n",
       "12335                   1            1              1  \n",
       "12336                   1            1              1  \n",
       "12337                   1            1              1  \n",
       "12338                   1            1              1  \n",
       "\n",
       "[12178 rows x 34 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop na rows\n",
    "test_df = df.dropna()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rz/d40gjr2505ddk3wl3mnzmvdm0000gn/T/ipykernel_15768/3448417843.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['Event_type'] = test_df['Event_type'].map(lambda x : 0 if x ==2  else x)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompNo</th>\n",
       "      <th>yyyy</th>\n",
       "      <th>mm</th>\n",
       "      <th>StkIndx</th>\n",
       "      <th>STInt</th>\n",
       "      <th>dtdlevel</th>\n",
       "      <th>dtdtrend</th>\n",
       "      <th>liqnonfinlevel</th>\n",
       "      <th>liqnonfintrend</th>\n",
       "      <th>ni2talevel</th>\n",
       "      <th>...</th>\n",
       "      <th>nan_count</th>\n",
       "      <th>liqnonfinlevel_notNA</th>\n",
       "      <th>liqnonfintrend_notNA</th>\n",
       "      <th>dtdlevel_notNA</th>\n",
       "      <th>dtdtrend_notNA</th>\n",
       "      <th>DTDmedianNonFin_notNA</th>\n",
       "      <th>Sector_Number_notNA</th>\n",
       "      <th>DTDmedianFin_notNA</th>\n",
       "      <th>sigma_notNA</th>\n",
       "      <th>StkIndx_notNA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26995</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>3.176331</td>\n",
       "      <td>-0.148469</td>\n",
       "      <td>0.280325</td>\n",
       "      <td>-0.146216</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26996</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26996</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27000</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27000</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12335</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12337</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12338</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12178 rows  34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CompNo  yyyy mm   StkIndx     STInt  dtdlevel  dtdtrend  \\\n",
       "0       26995  1990  1  0.106263  0.020305  3.176331 -0.148469   \n",
       "1       26996  1990  1  0.106263  0.020305  4.401022  0.054801   \n",
       "2       26996  1990  1  0.106263  0.020305  4.401022  0.054801   \n",
       "3       27000  1990  1  0.106263  0.020305  2.908823  0.056226   \n",
       "4       27000  1990  1  0.106263  0.020305  2.908823  0.056226   \n",
       "...       ...   ... ..       ...       ...       ...       ...   \n",
       "12334   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12335   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12336   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12337   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12338   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "\n",
       "       liqnonfinlevel  liqnonfintrend  ni2talevel  ...  nan_count  \\\n",
       "0            0.280325       -0.146216    0.002395  ...          3   \n",
       "1            0.951410        0.033574    0.002635  ...          3   \n",
       "2            0.951410        0.033574    0.002635  ...          3   \n",
       "3           -0.312616       -0.149733    0.004073  ...          3   \n",
       "4           -0.312616       -0.149733    0.004073  ...          3   \n",
       "...               ...             ...         ...  ...        ...   \n",
       "12334        0.673112       -0.089241    0.006030  ...          0   \n",
       "12335        0.673112       -0.089241    0.006030  ...          0   \n",
       "12336        0.673112       -0.089241    0.006030  ...          0   \n",
       "12337        0.673112       -0.089241    0.006030  ...          0   \n",
       "12338        0.673112       -0.089241    0.006030  ...          0   \n",
       "\n",
       "       liqnonfinlevel_notNA  liqnonfintrend_notNA  dtdlevel_notNA  \\\n",
       "0                         1                     1               0   \n",
       "1                         1                     1               0   \n",
       "2                         1                     1               0   \n",
       "3                         1                     1               0   \n",
       "4                         1                     1               0   \n",
       "...                     ...                   ...             ...   \n",
       "12334                     1                     1               1   \n",
       "12335                     1                     1               1   \n",
       "12336                     1                     1               1   \n",
       "12337                     1                     1               1   \n",
       "12338                     1                     1               1   \n",
       "\n",
       "       dtdtrend_notNA  DTDmedianNonFin_notNA  Sector_Number_notNA  \\\n",
       "0                   0                      0                    1   \n",
       "1                   0                      0                    1   \n",
       "2                   0                      0                    1   \n",
       "3                   0                      0                    1   \n",
       "4                   0                      0                    1   \n",
       "...               ...                    ...                  ...   \n",
       "12334               1                      1                    1   \n",
       "12335               1                      1                    1   \n",
       "12336               1                      1                    1   \n",
       "12337               1                      1                    1   \n",
       "12338               1                      1                    1   \n",
       "\n",
       "       DTDmedianFin_notNA  sigma_notNA  StkIndx_notNA  \n",
       "0                       1            1              1  \n",
       "1                       1            1              1  \n",
       "2                       1            1              1  \n",
       "3                       1            1              1  \n",
       "4                       1            1              1  \n",
       "...                   ...          ...            ...  \n",
       "12334                   1            1              1  \n",
       "12335                   1            1              1  \n",
       "12336                   1            1              1  \n",
       "12337                   1            1              1  \n",
       "12338                   1            1              1  \n",
       "\n",
       "[12178 rows x 34 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map event type\n",
    "test_df['Event_type'] = test_df['Event_type'].map(lambda x : 0 if x ==2  else x)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_df.drop(['Event_type', 'yyyy', 'mm', 'EventDate'], axis=1)  # Features\n",
    "y = test_df['Event_type']  # Target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 727, number of negative: 9015\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4108\n",
      "[LightGBM] [Info] Number of data points in the train set: 9742, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.889678 -> initscore=2.087452\n",
      "[LightGBM] [Info] Start training from score 2.087452\n"
     ]
    }
   ],
   "source": [
    "class_weights = {0:1, 1:100}\n",
    "model = LGBMClassifier( class_weight = class_weights)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.7540\n",
      "Testing accuracy 0.6773\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy {:.4f}'.format(model.score(X_train,y_train)))\n",
    "print('Testing accuracy {:.4f}'.format(model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.68      0.80      2254\n",
      "           1       0.14      0.67      0.24       182\n",
      "\n",
      "    accuracy                           0.68      2436\n",
      "   macro avg       0.55      0.67      0.52      2436\n",
      "weighted avg       0.90      0.68      0.75      2436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "push updated table to gbq, ie the table used for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompNo</th>\n",
       "      <th>StkIndx</th>\n",
       "      <th>STInt</th>\n",
       "      <th>dtdlevel</th>\n",
       "      <th>dtdtrend</th>\n",
       "      <th>liqnonfinlevel</th>\n",
       "      <th>liqnonfintrend</th>\n",
       "      <th>ni2talevel</th>\n",
       "      <th>ni2tatrend</th>\n",
       "      <th>sizelevel</th>\n",
       "      <th>...</th>\n",
       "      <th>nan_count</th>\n",
       "      <th>liqnonfinlevel_notNA</th>\n",
       "      <th>liqnonfintrend_notNA</th>\n",
       "      <th>dtdlevel_notNA</th>\n",
       "      <th>dtdtrend_notNA</th>\n",
       "      <th>DTDmedianNonFin_notNA</th>\n",
       "      <th>Sector_Number_notNA</th>\n",
       "      <th>DTDmedianFin_notNA</th>\n",
       "      <th>sigma_notNA</th>\n",
       "      <th>StkIndx_notNA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26995</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>3.176331</td>\n",
       "      <td>-0.148469</td>\n",
       "      <td>0.280325</td>\n",
       "      <td>-0.146216</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26996</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>-0.000224</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26996</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>-0.000224</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27000</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>1.248517</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27000</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>1.248517</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>27058</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>5.101596</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12335</th>\n",
       "      <td>27058</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>5.101596</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>27058</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>5.101596</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12337</th>\n",
       "      <td>27058</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>5.101596</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12338</th>\n",
       "      <td>27058</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>5.101596</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12178 rows  31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CompNo   StkIndx     STInt  dtdlevel  dtdtrend  liqnonfinlevel  \\\n",
       "0       26995  0.106263  0.020305  3.176331 -0.148469        0.280325   \n",
       "1       26996  0.106263  0.020305  4.401022  0.054801        0.951410   \n",
       "2       26996  0.106263  0.020305  4.401022  0.054801        0.951410   \n",
       "3       27000  0.106263  0.020305  2.908823  0.056226       -0.312616   \n",
       "4       27000  0.106263  0.020305  2.908823  0.056226       -0.312616   \n",
       "...       ...       ...       ...       ...       ...             ...   \n",
       "12334   27058  0.110445  0.009928  6.832332  2.326057        0.673112   \n",
       "12335   27058  0.110445  0.009928  6.832332  2.326057        0.673112   \n",
       "12336   27058  0.110445  0.009928  6.832332  2.326057        0.673112   \n",
       "12337   27058  0.110445  0.009928  6.832332  2.326057        0.673112   \n",
       "12338   27058  0.110445  0.009928  6.832332  2.326057        0.673112   \n",
       "\n",
       "       liqnonfintrend  ni2talevel  ni2tatrend  sizelevel  ...  nan_count  \\\n",
       "0           -0.146216    0.002395    0.001367   0.666644  ...          3   \n",
       "1            0.033574    0.002635   -0.000224   0.014110  ...          3   \n",
       "2            0.033574    0.002635   -0.000224   0.014110  ...          3   \n",
       "3           -0.149733    0.004073    0.004625   1.248517  ...          3   \n",
       "4           -0.149733    0.004073    0.004625   1.248517  ...          3   \n",
       "...               ...         ...         ...        ...  ...        ...   \n",
       "12334       -0.089241    0.006030    0.000547   5.101596  ...          0   \n",
       "12335       -0.089241    0.006030    0.000547   5.101596  ...          0   \n",
       "12336       -0.089241    0.006030    0.000547   5.101596  ...          0   \n",
       "12337       -0.089241    0.006030    0.000547   5.101596  ...          0   \n",
       "12338       -0.089241    0.006030    0.000547   5.101596  ...          0   \n",
       "\n",
       "       liqnonfinlevel_notNA  liqnonfintrend_notNA  dtdlevel_notNA  \\\n",
       "0                         1                     1               0   \n",
       "1                         1                     1               0   \n",
       "2                         1                     1               0   \n",
       "3                         1                     1               0   \n",
       "4                         1                     1               0   \n",
       "...                     ...                   ...             ...   \n",
       "12334                     1                     1               1   \n",
       "12335                     1                     1               1   \n",
       "12336                     1                     1               1   \n",
       "12337                     1                     1               1   \n",
       "12338                     1                     1               1   \n",
       "\n",
       "       dtdtrend_notNA  DTDmedianNonFin_notNA  Sector_Number_notNA  \\\n",
       "0                   0                      0                    1   \n",
       "1                   0                      0                    1   \n",
       "2                   0                      0                    1   \n",
       "3                   0                      0                    1   \n",
       "4                   0                      0                    1   \n",
       "...               ...                    ...                  ...   \n",
       "12334               1                      1                    1   \n",
       "12335               1                      1                    1   \n",
       "12336               1                      1                    1   \n",
       "12337               1                      1                    1   \n",
       "12338               1                      1                    1   \n",
       "\n",
       "       DTDmedianFin_notNA  sigma_notNA  StkIndx_notNA  \n",
       "0                       1            1              1  \n",
       "1                       1            1              1  \n",
       "2                       1            1              1  \n",
       "3                       1            1              1  \n",
       "4                       1            1              1  \n",
       "...                   ...          ...            ...  \n",
       "12334                   1            1              1  \n",
       "12335                   1            1              1  \n",
       "12336                   1            1              1  \n",
       "12337                   1            1              1  \n",
       "12338                   1            1              1  \n",
       "\n",
       "[12178 rows x 31 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = df.dropna()\n",
    "new_df = new_df.drop(['yyyy', 'mm', 'EventDate'], axis=1,)\n",
    "new_df['Event_type'] = new_df['Event_type'].map(lambda x : 0 if x ==2  else x)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_gbq(destination_table=f'{project_id}.{dataset_id}.filled_table', project_id=project_id, if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
