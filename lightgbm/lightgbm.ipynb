{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import optuna as optuna\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompanyNumber</th>\n",
       "      <th>yyyy</th>\n",
       "      <th>mm</th>\n",
       "      <th>DTDmedianFin</th>\n",
       "      <th>DTDmedianNonFin</th>\n",
       "      <th>dummy297fin</th>\n",
       "      <th>EventDate</th>\n",
       "      <th>EventDate_string</th>\n",
       "      <th>Duration</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>...</th>\n",
       "      <th>equity_ratio</th>\n",
       "      <th>financial_leverage_ratio</th>\n",
       "      <th>cashflow_to_debt_ratio</th>\n",
       "      <th>net_profit_margin</th>\n",
       "      <th>asset_turnover</th>\n",
       "      <th>receivables_turnover</th>\n",
       "      <th>day_sales_outstanding</th>\n",
       "      <th>working_capital_turnover</th>\n",
       "      <th>price_to_earnings</th>\n",
       "      <th>retention_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26980</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.190874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-05-01 00:00:00+00:00</td>\n",
       "      <td>2000 05</td>\n",
       "      <td>4501 days 00:00:00</td>\n",
       "      <td>1988-01-04 00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>8.653470</td>\n",
       "      <td>0.115561</td>\n",
       "      <td>0.353320</td>\n",
       "      <td>0.496181</td>\n",
       "      <td>0.500283</td>\n",
       "      <td>0.439043</td>\n",
       "      <td>2.277680</td>\n",
       "      <td>6.329094</td>\n",
       "      <td>-10.552301</td>\n",
       "      <td>5.626957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26995</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.863172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-12-01 00:00:00+00:00</td>\n",
       "      <td>2000 12</td>\n",
       "      <td>4715 days 00:00:00</td>\n",
       "      <td>1988-01-04 00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>5.227711</td>\n",
       "      <td>0.191288</td>\n",
       "      <td>0.270503</td>\n",
       "      <td>0.309987</td>\n",
       "      <td>0.252043</td>\n",
       "      <td>5.105960</td>\n",
       "      <td>0.195850</td>\n",
       "      <td>-30.840000</td>\n",
       "      <td>98.727273</td>\n",
       "      <td>0.127184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27003</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.257787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-06-01 00:00:00+00:00</td>\n",
       "      <td>2000 06</td>\n",
       "      <td>4532 days 00:00:00</td>\n",
       "      <td>1988-01-04 00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.758440</td>\n",
       "      <td>0.362524</td>\n",
       "      <td>0.040856</td>\n",
       "      <td>0.553460</td>\n",
       "      <td>0.471932</td>\n",
       "      <td>0.027663</td>\n",
       "      <td>42.234378</td>\n",
       "      <td>1.449149</td>\n",
       "      <td>42.256637</td>\n",
       "      <td>13.899441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27058</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.599548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-03-01 00:00:00+00:00</td>\n",
       "      <td>2000 03</td>\n",
       "      <td>4440 days 00:00:00</td>\n",
       "      <td>1988-01-04 00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977792</td>\n",
       "      <td>1.022712</td>\n",
       "      <td>0.154983</td>\n",
       "      <td>0.351704</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.866751</td>\n",
       "      <td>0.348827</td>\n",
       "      <td>0.677361</td>\n",
       "      <td>14.625000</td>\n",
       "      <td>5.160377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27071</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.318686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-04-01 00:00:00+00:00</td>\n",
       "      <td>2000 04</td>\n",
       "      <td>4471 days 00:00:00</td>\n",
       "      <td>1988-01-04 00:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.883145</td>\n",
       "      <td>1.132316</td>\n",
       "      <td>0.307483</td>\n",
       "      <td>0.303481</td>\n",
       "      <td>0.541856</td>\n",
       "      <td>1.503662</td>\n",
       "      <td>0.665043</td>\n",
       "      <td>3.567568</td>\n",
       "      <td>17.829861</td>\n",
       "      <td>14.014418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CompanyNumber    yyyy  mm  DTDmedianFin  DTDmedianNonFin  dummy297fin  \\\n",
       "0          26980  2000.0   5           0.0         2.190874          0.0   \n",
       "1          26995  2000.0  12           0.0         1.863172          0.0   \n",
       "2          27003  2000.0   6           0.0         2.257787          0.0   \n",
       "3          27058  2000.0   3           0.0         2.599548          0.0   \n",
       "4          27071  2000.0   4           0.0         2.318686          0.0   \n",
       "\n",
       "                  EventDate EventDate_string            Duration  \\\n",
       "0 2000-05-01 00:00:00+00:00          2000 05  4501 days 00:00:00   \n",
       "1 2000-12-01 00:00:00+00:00          2000 12  4715 days 00:00:00   \n",
       "2 2000-06-01 00:00:00+00:00          2000 06  4532 days 00:00:00   \n",
       "3 2000-03-01 00:00:00+00:00          2000 03  4440 days 00:00:00   \n",
       "4 2000-04-01 00:00:00+00:00          2000 04  4471 days 00:00:00   \n",
       "\n",
       "                  StartDate  ...  equity_ratio financial_leverage_ratio  \\\n",
       "0 1988-01-04 00:00:00+00:00  ...      8.653470                 0.115561   \n",
       "1 1988-01-04 00:00:00+00:00  ...      5.227711                 0.191288   \n",
       "2 1988-01-04 00:00:00+00:00  ...      2.758440                 0.362524   \n",
       "3 1988-01-04 00:00:00+00:00  ...      0.977792                 1.022712   \n",
       "4 1988-01-04 00:00:00+00:00  ...      0.883145                 1.132316   \n",
       "\n",
       "  cashflow_to_debt_ratio net_profit_margin  asset_turnover  \\\n",
       "0               0.353320          0.496181        0.500283   \n",
       "1               0.270503          0.309987        0.252043   \n",
       "2               0.040856          0.553460        0.471932   \n",
       "3               0.154983          0.351704        1.000000   \n",
       "4               0.307483          0.303481        0.541856   \n",
       "\n",
       "   receivables_turnover  day_sales_outstanding  working_capital_turnover  \\\n",
       "0              0.439043               2.277680                  6.329094   \n",
       "1              5.105960               0.195850                -30.840000   \n",
       "2              0.027663              42.234378                  1.449149   \n",
       "3              2.866751               0.348827                  0.677361   \n",
       "4              1.503662               0.665043                  3.567568   \n",
       "\n",
       "   price_to_earnings  retention_ratio  \n",
       "0         -10.552301         5.626957  \n",
       "1          98.727273         0.127184  \n",
       "2          42.256637        13.899441  \n",
       "3          14.625000         5.160377  \n",
       "4          17.829861        14.014418  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the path to your pickle file\n",
    "file_path = \"../dataset/merged_data.pkl\"\n",
    "\n",
    "# Open the pickle file for reading\n",
    "with open(file_path, 'rb') as file:\n",
    "    # Load the data from the pickle file\n",
    "    data = pickle.load(file)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df shape: (564102, 65)\n",
      "val_df shape: (8390, 65)\n",
      "train_df shape: (564102, 65)\n",
      "test_df shape: (6733, 65)\n"
     ]
    }
   ],
   "source": [
    "selected_features = [\n",
    "        'DTDmedianFin', 'DTDmedianNonFin', 'dummy297fin', 'sic', 'atq',\n",
    "        # 'ltq', 'dlcq', 'txditcq', 'cheq', \n",
    "        # 'lctq', 'actq', 'invtq', 'intanq',\n",
    "        # 'oibdpq', 'txpq', 'piq', 'niq', 'revtq', 'cogsq', 'oancfy', 'ivncfy',\n",
    "        # 'fincfy', 'dltisy', 'dltry', 'ceqq', 'dpq', 'saleq', 'saley', 'invchy',\n",
    "        # 'rectq', 'wcapq', 'prccq', 'epsf12', 'cshoq', 'dvy', 'req', 'dlttq',\n",
    "        # 'chechy', \n",
    "        'current_ratio', 'quick_ratio',\n",
    "        'cash_ratio', 'net_working_capital', 'debt_ratio',\n",
    "        'debt_to_equity_ratio', 'equity_ratio', 'financial_leverage_ratio',\n",
    "        'cashflow_to_debt_ratio', 'net_profit_margin', 'asset_turnover',\n",
    "        'receivables_turnover', 'day_sales_outstanding',\n",
    "        'working_capital_turnover', 'price_to_earnings', 'retention_ratio',\n",
    "        ]\n",
    "\n",
    "\n",
    "data['Duration'] = data['Duration'].str.extract(r'(\\d+) days').astype(int)\n",
    "data['sic'] = data['sic'].astype(float)\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "train_df = data[data['Duration'] > 653]\n",
    "test_df = data[data['Duration'] <= 653]\n",
    "val_df = train_df[train_df[\"Duration\"] < 1384]\n",
    "train_df = train_df[train_df[\"Duration\"] >= 1384]\n",
    "print(\"train_df shape:\", train_df.shape)\n",
    "print(\"val_df shape:\", val_df.shape)\n",
    "print(\"train_df shape:\", train_df.shape)\n",
    "print(\"test_df shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(564102, 21)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df[selected_features] # Features\n",
    "y_train = train_df['EventType']  # Target\n",
    "X_test = test_df[selected_features] # Features\n",
    "y_test = test_df['EventType']  # Target\n",
    "X_val = val_df[selected_features] # Features\n",
    "y_val = val_df['EventType']  # Target\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:36:48,163] A new study created in memory with name: no-name-0fb5c38f-3be8-4100-954f-70e448288d2c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:37:12,871] Trial 0 finished with value: 0.5773373005155185 and parameters: {'num_leaves': 255, 'lambda_l1': 8.848549501400608e-06, 'lambda_l2': 1.674527841748936e-08, 'feature_fraction': 0.8166362043109641, 'bagging_fraction': 0.4934501461772914, 'bagging_freq': 6, 'min_child_samples': 86}. Best is trial 0 with value: 0.5773373005155185.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011595 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:37:28,588] Trial 1 finished with value: 0.6328450685204975 and parameters: {'num_leaves': 62, 'lambda_l1': 6.764735254713837e-08, 'lambda_l2': 1.6694670818941871e-07, 'feature_fraction': 0.964337502410802, 'bagging_fraction': 0.5662024339571684, 'bagging_freq': 5, 'min_child_samples': 72}. Best is trial 1 with value: 0.6328450685204975.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039628 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:37:50,406] Trial 2 finished with value: 0.6004211339056097 and parameters: {'num_leaves': 168, 'lambda_l1': 1.3756468446790676e-05, 'lambda_l2': 0.022473344613868816, 'feature_fraction': 0.6957790721926698, 'bagging_fraction': 0.6774855364508192, 'bagging_freq': 7, 'min_child_samples': 35}. Best is trial 1 with value: 0.6328450685204975.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:38:15,792] Trial 3 finished with value: 0.6238910189414846 and parameters: {'num_leaves': 176, 'lambda_l1': 0.013776942049153066, 'lambda_l2': 0.7166178303872184, 'feature_fraction': 0.8778389391744982, 'bagging_fraction': 0.52583924440858, 'bagging_freq': 4, 'min_child_samples': 96}. Best is trial 1 with value: 0.6328450685204975.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:38:32,642] Trial 4 finished with value: 0.6350446821752762 and parameters: {'num_leaves': 80, 'lambda_l1': 0.02764245826286201, 'lambda_l2': 0.20629540863895018, 'feature_fraction': 0.4198597536484228, 'bagging_fraction': 0.7576208927329298, 'bagging_freq': 2, 'min_child_samples': 100}. Best is trial 4 with value: 0.6350446821752762.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038962 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:38:49,499] Trial 5 finished with value: 0.6600719846871073 and parameters: {'num_leaves': 93, 'lambda_l1': 0.00012651908162373098, 'lambda_l2': 8.852259957228657e-08, 'feature_fraction': 0.9453181070589197, 'bagging_fraction': 0.4062107782917877, 'bagging_freq': 6, 'min_child_samples': 83}. Best is trial 5 with value: 0.6600719846871073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:39:13,558] Trial 6 finished with value: 0.6216334241459891 and parameters: {'num_leaves': 154, 'lambda_l1': 2.2727519675366172e-08, 'lambda_l2': 0.0016443818068255446, 'feature_fraction': 0.752277422708075, 'bagging_fraction': 0.7201715234936614, 'bagging_freq': 4, 'min_child_samples': 68}. Best is trial 5 with value: 0.6600719846871073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040668 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:39:29,553] Trial 7 finished with value: 0.6192517937456697 and parameters: {'num_leaves': 102, 'lambda_l1': 1.5089070797760153e-07, 'lambda_l2': 7.682199934897833e-05, 'feature_fraction': 0.7665134262422664, 'bagging_fraction': 0.6079288870526296, 'bagging_freq': 5, 'min_child_samples': 18}. Best is trial 5 with value: 0.6600719846871073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:39:56,786] Trial 8 finished with value: 0.6303276594995126 and parameters: {'num_leaves': 179, 'lambda_l1': 0.00024381665221252617, 'lambda_l2': 0.2871084972757033, 'feature_fraction': 0.8281638913035116, 'bagging_fraction': 0.9490911650824032, 'bagging_freq': 2, 'min_child_samples': 43}. Best is trial 5 with value: 0.6600719846871073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:40:10,790] Trial 9 finished with value: 0.6372575067228765 and parameters: {'num_leaves': 42, 'lambda_l1': 0.16075288461633433, 'lambda_l2': 5.966479517420078e-08, 'feature_fraction': 0.587162602867796, 'bagging_fraction': 0.9662025603354432, 'bagging_freq': 2, 'min_child_samples': 64}. Best is trial 5 with value: 0.6600719846871073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:40:18,997] Trial 10 finished with value: 0.6531803022652277 and parameters: {'num_leaves': 16, 'lambda_l1': 1.8534789250638466, 'lambda_l2': 7.221883250436221e-06, 'feature_fraction': 0.9765203835143031, 'bagging_fraction': 0.41228689376430294, 'bagging_freq': 7, 'min_child_samples': 11}. Best is trial 5 with value: 0.6600719846871073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Melissa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-10-17 00:40:29,709] Trial 11 finished with value: 0.6325881900489683 and parameters: {'num_leaves': 22, 'lambda_l1': 9.941025553712766, 'lambda_l2': 2.494118055192818e-06, 'feature_fraction': 0.9719941504319224, 'bagging_fraction': 0.40185571604333503, 'bagging_freq': 7, 'min_child_samples': 12}. Best is trial 5 with value: 0.6600719846871073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040541 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Melissa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-10-17 00:40:35,815] Trial 12 finished with value: 0.6407488521202015 and parameters: {'num_leaves': 3, 'lambda_l1': 8.161711163047116, 'lambda_l2': 3.3552342298852338e-06, 'feature_fraction': 0.9834456156681786, 'bagging_fraction': 0.4098634310767354, 'bagging_freq': 6, 'min_child_samples': 28}. Best is trial 5 with value: 0.6600719846871073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:40:57,894] Trial 13 finished with value: 0.6211314102187724 and parameters: {'num_leaves': 116, 'lambda_l1': 0.001284707591016972, 'lambda_l2': 1.6988916305163855e-06, 'feature_fraction': 0.9066619564697743, 'bagging_fraction': 0.4684274419831032, 'bagging_freq': 7, 'min_child_samples': 52}. Best is trial 5 with value: 0.6600719846871073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044292 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:41:11,759] Trial 14 finished with value: 0.5639297121786817 and parameters: {'num_leaves': 67, 'lambda_l1': 0.7772283494281007, 'lambda_l2': 0.00012400181150869953, 'feature_fraction': 0.9950715354584287, 'bagging_fraction': 0.40908788275874014, 'bagging_freq': 6, 'min_child_samples': 6}. Best is trial 5 with value: 0.6600719846871073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:41:41,346] Trial 15 finished with value: 0.5785739868713082 and parameters: {'num_leaves': 223, 'lambda_l1': 0.001080725993890771, 'lambda_l2': 6.896772024642558, 'feature_fraction': 0.8903688749962447, 'bagging_fraction': 0.4884489863977601, 'bagging_freq': 5, 'min_child_samples': 78}. Best is trial 5 with value: 0.6600719846871073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:41:55,208] Trial 16 finished with value: 0.6077957478539638 and parameters: {'num_leaves': 40, 'lambda_l1': 0.1577935056360259, 'lambda_l2': 3.465000346759426e-07, 'feature_fraction': 0.9240731280275694, 'bagging_fraction': 0.5794396594307709, 'bagging_freq': 3, 'min_child_samples': 54}. Best is trial 5 with value: 0.6600719846871073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:42:19,149] Trial 17 finished with value: 0.6129912984252616 and parameters: {'num_leaves': 136, 'lambda_l1': 9.325959281352338e-05, 'lambda_l2': 1.0090910718657916e-08, 'feature_fraction': 0.9994654179500433, 'bagging_fraction': 0.46622917832913435, 'bagging_freq': 6, 'min_child_samples': 23}. Best is trial 5 with value: 0.6600719846871073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058900 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:42:37,603] Trial 18 finished with value: 0.6224796552250549 and parameters: {'num_leaves': 89, 'lambda_l1': 0.013699823106648154, 'lambda_l2': 1.5135174591813768e-05, 'feature_fraction': 0.8445653191203151, 'bagging_fraction': 0.6361536850111589, 'bagging_freq': 7, 'min_child_samples': 84}. Best is trial 5 with value: 0.6600719846871073.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:42:45,198] Trial 19 finished with value: 0.6659794555937856 and parameters: {'num_leaves': 7, 'lambda_l1': 0.002843135877679122, 'lambda_l2': 7.031181463665935e-07, 'feature_fraction': 0.921038196572129, 'bagging_fraction': 0.5428341941679213, 'bagging_freq': 4, 'min_child_samples': 56}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:42:59,418] Trial 20 finished with value: 0.5901232429512547 and parameters: {'num_leaves': 45, 'lambda_l1': 2.106284088130465e-06, 'lambda_l2': 3.365700174117335e-07, 'feature_fraction': 0.9204421901252421, 'bagging_fraction': 0.5535801480526505, 'bagging_freq': 1, 'min_child_samples': 56}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:43:08,976] Trial 21 finished with value: 0.6186316157215497 and parameters: {'num_leaves': 13, 'lambda_l1': 0.0010830719987537953, 'lambda_l2': 7.96341133042148e-06, 'feature_fraction': 0.9270728295103073, 'bagging_fraction': 0.4395582440273903, 'bagging_freq': 3, 'min_child_samples': 43}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:43:19,182] Trial 22 finished with value: 0.6199886679897131 and parameters: {'num_leaves': 27, 'lambda_l1': 0.00011375553131432152, 'lambda_l2': 9.291520689208298e-07, 'feature_fraction': 0.8608210562675284, 'bagging_fraction': 0.5122901755863825, 'bagging_freq': 5, 'min_child_samples': 38}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Melissa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-10-17 00:43:26,243] Trial 23 finished with value: 0.6511957325880433 and parameters: {'num_leaves': 3, 'lambda_l1': 0.0028741230706235393, 'lambda_l2': 6.842371959208863e-08, 'feature_fraction': 0.9380742495226979, 'bagging_fraction': 0.4528184059039707, 'bagging_freq': 3, 'min_child_samples': 62}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:43:43,632] Trial 24 finished with value: 0.5952182439494111 and parameters: {'num_leaves': 63, 'lambda_l1': 1.2539018625899712, 'lambda_l2': 2.1351929726813145e-05, 'feature_fraction': 0.9434896477480176, 'bagging_fraction': 0.529073557313593, 'bagging_freq': 4, 'min_child_samples': 93}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:44:06,702] Trial 25 finished with value: 0.5896623295794826 and parameters: {'num_leaves': 128, 'lambda_l1': 0.005091209144517962, 'lambda_l2': 6.712807052539195e-07, 'feature_fraction': 0.8759164566099206, 'bagging_fraction': 0.4442163845388642, 'bagging_freq': 6, 'min_child_samples': 73}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041125 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:44:18,035] Trial 26 finished with value: 0.6023726763507404 and parameters: {'num_leaves': 32, 'lambda_l1': 0.06187596443400974, 'lambda_l2': 8.895396195296247e-08, 'feature_fraction': 0.9583776342589545, 'bagging_fraction': 0.4039732405505531, 'bagging_freq': 5, 'min_child_samples': 29}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039186 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:44:39,299] Trial 27 finished with value: 0.6115769989548716 and parameters: {'num_leaves': 94, 'lambda_l1': 0.0003200206200296573, 'lambda_l2': 4.411190968794334e-06, 'feature_fraction': 0.9991844486246696, 'bagging_fraction': 0.499986028667056, 'bagging_freq': 7, 'min_child_samples': 49}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:45:04,338] Trial 28 finished with value: 0.6313104031377339 and parameters: {'num_leaves': 58, 'lambda_l1': 0.007275604965022488, 'lambda_l2': 2.405670032219769e-05, 'feature_fraction': 0.8853854757021699, 'bagging_fraction': 0.5437113285507925, 'bagging_freq': 6, 'min_child_samples': 79}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116841 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:45:57,777] Trial 29 finished with value: 0.5983360146552837 and parameters: {'num_leaves': 197, 'lambda_l1': 2.1103349514661812e-05, 'lambda_l2': 2.8970344131907102e-08, 'feature_fraction': 0.8118071283339129, 'bagging_fraction': 0.4851369892549427, 'bagging_freq': 4, 'min_child_samples': 89}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.123572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:46:57,250] Trial 30 finished with value: 0.6055976020761652 and parameters: {'num_leaves': 241, 'lambda_l1': 2.2039338466533283e-06, 'lambda_l2': 1.1958380695118282e-08, 'feature_fraction': 0.9363123580221273, 'bagging_fraction': 0.4475547314058349, 'bagging_freq': 7, 'min_child_samples': 60}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:47:12,121] Trial 31 finished with value: 0.6514445377361814 and parameters: {'num_leaves': 6, 'lambda_l1': 0.002934988111654524, 'lambda_l2': 7.528201822761388e-08, 'feature_fraction': 0.9419879042844719, 'bagging_fraction': 0.4508998385686647, 'bagging_freq': 3, 'min_child_samples': 62}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.100839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:47:30,597] Trial 32 finished with value: 0.6020820367086674 and parameters: {'num_leaves': 18, 'lambda_l1': 0.0025474457992537966, 'lambda_l2': 2.7030287883763934e-07, 'feature_fraction': 0.9614365843305015, 'bagging_fraction': 0.4992050864871791, 'bagging_freq': 3, 'min_child_samples': 69}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:47:58,624] Trial 33 finished with value: 0.6298623424968 and parameters: {'num_leaves': 51, 'lambda_l1': 0.05145890879656844, 'lambda_l2': 1.687373900993419e-07, 'feature_fraction': 0.9065820874338107, 'bagging_fraction': 0.4364592905998094, 'bagging_freq': 4, 'min_child_samples': 78}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:48:36,595] Trial 34 finished with value: 0.6162455816902896 and parameters: {'num_leaves': 76, 'lambda_l1': 0.0005717325665706402, 'lambda_l2': 4.429137857007847e-08, 'feature_fraction': 0.9572302193401644, 'bagging_fraction': 0.46384365105386427, 'bagging_freq': 3, 'min_child_samples': 46}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Melissa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-10-17 00:48:50,783] Trial 35 finished with value: 0.631975351409749 and parameters: {'num_leaves': 5, 'lambda_l1': 0.007631224820124013, 'lambda_l2': 1.0107898059755474e-06, 'feature_fraction': 0.8589707514326567, 'bagging_fraction': 0.5218081467481995, 'bagging_freq': 5, 'min_child_samples': 35}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097668 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:49:12,530] Trial 36 finished with value: 0.6371738377350071 and parameters: {'num_leaves': 28, 'lambda_l1': 4.093832707800598e-05, 'lambda_l2': 1.1063134888535367e-07, 'feature_fraction': 0.8984098727867406, 'bagging_fraction': 0.5841145395855154, 'bagging_freq': 4, 'min_child_samples': 85}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113636 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:49:33,007] Trial 37 finished with value: 0.603553583381284 and parameters: {'num_leaves': 19, 'lambda_l1': 0.015774183397856946, 'lambda_l2': 2.8475247874863225e-08, 'feature_fraction': 0.8007436348385568, 'bagging_fraction': 0.43228993985237896, 'bagging_freq': 1, 'min_child_samples': 73}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:50:00,993] Trial 38 finished with value: 0.6197288537642238 and parameters: {'num_leaves': 36, 'lambda_l1': 0.0003371979148353188, 'lambda_l2': 5.252157483014775e-07, 'feature_fraction': 0.9704567599690908, 'bagging_fraction': 0.4839273230017711, 'bagging_freq': 2, 'min_child_samples': 67}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:50:28,994] Trial 39 finished with value: 0.6469601735617742 and parameters: {'num_leaves': 52, 'lambda_l1': 0.002471989222248388, 'lambda_l2': 1.3467898536674991e-06, 'feature_fraction': 0.8537380028614322, 'bagging_fraction': 0.528425514640037, 'bagging_freq': 6, 'min_child_samples': 57}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:51:13,856] Trial 40 finished with value: 0.5628522904752398 and parameters: {'num_leaves': 150, 'lambda_l1': 0.02651592178297122, 'lambda_l2': 1.461513794049412e-07, 'feature_fraction': 0.8313459627036927, 'bagging_fraction': 0.40264661723705114, 'bagging_freq': 4, 'min_child_samples': 99}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.114649 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Melissa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-10-17 00:51:27,126] Trial 41 finished with value: 0.6504250971734561 and parameters: {'num_leaves': 3, 'lambda_l1': 0.005414088608178457, 'lambda_l2': 5.5009527838855386e-08, 'feature_fraction': 0.9323665087490093, 'bagging_fraction': 0.45031669135874974, 'bagging_freq': 3, 'min_child_samples': 62}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:51:44,446] Trial 42 finished with value: 0.651768204610308 and parameters: {'num_leaves': 14, 'lambda_l1': 0.002701466022416881, 'lambda_l2': 2.8516264069820668e-08, 'feature_fraction': 0.9437501566352634, 'bagging_fraction': 0.43010749397537296, 'bagging_freq': 2, 'min_child_samples': 63}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.100519 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:52:02,426] Trial 43 finished with value: 0.6497263877308971 and parameters: {'num_leaves': 16, 'lambda_l1': 0.0001301685961494402, 'lambda_l2': 1.969169772637049e-08, 'feature_fraction': 0.972238412272505, 'bagging_fraction': 0.427587098654659, 'bagging_freq': 2, 'min_child_samples': 49}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:52:41,221] Trial 44 finished with value: 0.6282139166480736 and parameters: {'num_leaves': 110, 'lambda_l1': 0.0013607051596864442, 'lambda_l2': 1.9578541205918575e-07, 'feature_fraction': 0.8979206418941259, 'bagging_fraction': 0.4711009447127198, 'bagging_freq': 2, 'min_child_samples': 67}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:53:04,596] Trial 45 finished with value: 0.6057642061134141 and parameters: {'num_leaves': 77, 'lambda_l1': 0.0005586081039137813, 'lambda_l2': 2.5798425116487993e-06, 'feature_fraction': 0.9678751813885929, 'bagging_fraction': 0.416329695171682, 'bagging_freq': 1, 'min_child_samples': 58}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044837 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:53:23,672] Trial 46 finished with value: 0.5926934955435256 and parameters: {'num_leaves': 31, 'lambda_l1': 0.23409485950421782, 'lambda_l2': 5.43929772680884e-07, 'feature_fraction': 0.887709900754135, 'bagging_fraction': 0.4971475912923777, 'bagging_freq': 3, 'min_child_samples': 39}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.123486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:53:42,415] Trial 47 finished with value: 0.6410893995796001 and parameters: {'num_leaves': 13, 'lambda_l1': 0.020733432450944625, 'lambda_l2': 2.398340116484009e-08, 'feature_fraction': 0.9876184502082165, 'bagging_fraction': 0.42694129270587056, 'bagging_freq': 2, 'min_child_samples': 14}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:53:55,297] Trial 48 finished with value: 0.6577637481357963 and parameters: {'num_leaves': 39, 'lambda_l1': 4.307283601762783e-05, 'lambda_l2': 0.0007864402916544991, 'feature_fraction': 0.9542958808968313, 'bagging_fraction': 0.400109841229817, 'bagging_freq': 7, 'min_child_samples': 52}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:54:08,526] Trial 49 finished with value: 0.5713021243115657 and parameters: {'num_leaves': 68, 'lambda_l1': 8.261638196171414e-06, 'lambda_l2': 0.0007713967472312117, 'feature_fraction': 0.9141759323837109, 'bagging_fraction': 0.4100129351170536, 'bagging_freq': 7, 'min_child_samples': 5}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:54:22,485] Trial 50 finished with value: 0.5995646276876828 and parameters: {'num_leaves': 42, 'lambda_l1': 6.002921380151758e-05, 'lambda_l2': 0.005287723925389579, 'feature_fraction': 0.9983622732620583, 'bagging_fraction': 0.5608756626153025, 'bagging_freq': 7, 'min_child_samples': 28}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:54:31,851] Trial 51 finished with value: 0.6019499277804525 and parameters: {'num_leaves': 23, 'lambda_l1': 0.00017564391075440845, 'lambda_l2': 0.00019494467142109346, 'feature_fraction': 0.9482935698947901, 'bagging_fraction': 0.4653185751889968, 'bagging_freq': 7, 'min_child_samples': 53}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:54:39,309] Trial 52 finished with value: 0.5817284545016852 and parameters: {'num_leaves': 12, 'lambda_l1': 0.0005452324079024696, 'lambda_l2': 1.0310753962202098e-08, 'feature_fraction': 0.9169099361673032, 'bagging_fraction': 0.40010009592839674, 'bagging_freq': 6, 'min_child_samples': 82}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046170 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:54:52,801] Trial 53 finished with value: 0.6240759714409855 and parameters: {'num_leaves': 47, 'lambda_l1': 3.039931174787498e-05, 'lambda_l2': 8.973832158225646e-08, 'feature_fraction': 0.9486741636354854, 'bagging_fraction': 0.42936499361843183, 'bagging_freq': 7, 'min_child_samples': 71}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:55:04,588] Trial 54 finished with value: 0.5966707082212854 and parameters: {'num_leaves': 34, 'lambda_l1': 0.00015051227759378417, 'lambda_l2': 6.985052024174032e-05, 'feature_fraction': 0.977924576942481, 'bagging_fraction': 0.4778262296182436, 'bagging_freq': 5, 'min_child_samples': 90}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:55:11,971] Trial 55 finished with value: 0.6244620230867691 and parameters: {'num_leaves': 9, 'lambda_l1': 0.0015616940986441603, 'lambda_l2': 2.6746630688030217e-07, 'feature_fraction': 0.8757746986419908, 'bagging_fraction': 0.46479515744210625, 'bagging_freq': 6, 'min_child_samples': 63}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:55:23,838] Trial 56 finished with value: 0.5909203001514849 and parameters: {'num_leaves': 26, 'lambda_l1': 3.590205588996513, 'lambda_l2': 5.119086916125657e-06, 'feature_fraction': 0.9161743223534007, 'bagging_fraction': 0.4282023953057034, 'bagging_freq': 7, 'min_child_samples': 75}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:55:39,867] Trial 57 finished with value: 0.6019440562725319 and parameters: {'num_leaves': 56, 'lambda_l1': 0.23842618979162036, 'lambda_l2': 4.481469665090337e-08, 'feature_fraction': 0.9378350720257408, 'bagging_fraction': 0.5104801270187538, 'bagging_freq': 3, 'min_child_samples': 22}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:55:54,836] Trial 58 finished with value: 0.5997158190166398 and parameters: {'num_leaves': 39, 'lambda_l1': 6.647166248108603e-05, 'lambda_l2': 1.45502158238424e-06, 'feature_fraction': 0.974680811849861, 'bagging_fraction': 0.44912496775458494, 'bagging_freq': 2, 'min_child_samples': 50}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047757 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:56:12,525] Trial 59 finished with value: 0.6001539802952194 and parameters: {'num_leaves': 90, 'lambda_l1': 0.05975752979859104, 'lambda_l2': 4.538304052483222e-07, 'feature_fraction': 0.6977673779489932, 'bagging_fraction': 0.4858894167359025, 'bagging_freq': 5, 'min_child_samples': 44}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:56:30,611] Trial 60 finished with value: 0.6526430592904869 and parameters: {'num_leaves': 69, 'lambda_l1': 0.0002659524386234414, 'lambda_l2': 2.104292664580717e-06, 'feature_fraction': 0.9492737411550455, 'bagging_fraction': 0.4257351225542562, 'bagging_freq': 4, 'min_child_samples': 65}. Best is trial 19 with value: 0.6659794555937856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:56:46,779] Trial 61 finished with value: 0.6775221062273213 and parameters: {'num_leaves': 69, 'lambda_l1': 0.00019666115428383546, 'lambda_l2': 2.704552942274959e-06, 'feature_fraction': 0.9529078374544686, 'bagging_fraction': 0.40073920920252026, 'bagging_freq': 4, 'min_child_samples': 65}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:57:03,797] Trial 62 finished with value: 0.6160848491609615 and parameters: {'num_leaves': 71, 'lambda_l1': 0.00028216893863732494, 'lambda_l2': 7.852986669185877e-06, 'feature_fraction': 0.984345679967148, 'bagging_fraction': 0.40272105870455893, 'bagging_freq': 4, 'min_child_samples': 65}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:57:21,865] Trial 63 finished with value: 0.5855867691440516 and parameters: {'num_leaves': 84, 'lambda_l1': 9.595326215530556e-05, 'lambda_l2': 2.5099255843356037e-06, 'feature_fraction': 0.9255999294115836, 'bagging_fraction': 0.4228336342175868, 'bagging_freq': 4, 'min_child_samples': 58}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:57:41,406] Trial 64 finished with value: 0.6035822069823973 and parameters: {'num_leaves': 99, 'lambda_l1': 1.1973319043916332e-05, 'lambda_l2': 9.20189518625035e-07, 'feature_fraction': 0.9563167892150433, 'bagging_fraction': 0.4209460003335568, 'bagging_freq': 6, 'min_child_samples': 54}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:58:01,610] Trial 65 finished with value: 0.5935749556701152 and parameters: {'num_leaves': 107, 'lambda_l1': 0.000628643716778503, 'lambda_l2': 1.3156125753210866e-05, 'feature_fraction': 0.9003974050113023, 'bagging_fraction': 0.4457624362797119, 'bagging_freq': 5, 'min_child_samples': 76}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:58:17,708] Trial 66 finished with value: 0.6622158190166398 and parameters: {'num_leaves': 61, 'lambda_l1': 0.00024446212198844905, 'lambda_l2': 2.077958149497257e-06, 'feature_fraction': 0.998370800537139, 'bagging_fraction': 0.466683073612954, 'bagging_freq': 4, 'min_child_samples': 70}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:58:35,490] Trial 67 finished with value: 0.6503172082154138 and parameters: {'num_leaves': 63, 'lambda_l1': 0.00022047533967830478, 'lambda_l2': 1.7686315026032097e-06, 'feature_fraction': 0.9776853451767668, 'bagging_fraction': 0.46349545237936207, 'bagging_freq': 4, 'min_child_samples': 70}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:59:01,544] Trial 68 finished with value: 0.6048842138638044 and parameters: {'num_leaves': 122, 'lambda_l1': 4.8588109482060336e-05, 'lambda_l2': 4.09937808312094e-06, 'feature_fraction': 0.9982829305179011, 'bagging_fraction': 0.5397835872485912, 'bagging_freq': 4, 'min_child_samples': 83}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:59:21,401] Trial 69 finished with value: 0.6508008736803785 and parameters: {'num_leaves': 82, 'lambda_l1': 2.6315122048772044e-05, 'lambda_l2': 3.186120831567978e-05, 'feature_fraction': 0.9600693528624872, 'bagging_fraction': 0.5124031068664412, 'bagging_freq': 4, 'min_child_samples': 80}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:59:35,575] Trial 70 finished with value: 0.5675164695797176 and parameters: {'num_leaves': 58, 'lambda_l1': 0.0008077459864476171, 'lambda_l2': 8.100655469575879e-07, 'feature_fraction': 0.9299057448510738, 'bagging_fraction': 0.41279001938305077, 'bagging_freq': 5, 'min_child_samples': 10}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:59:49,375] Trial 71 finished with value: 0.6412251782002654 and parameters: {'num_leaves': 47, 'lambda_l1': 0.00033444533846945635, 'lambda_l2': 7.4898826496373365e-06, 'feature_fraction': 0.9533032577133305, 'bagging_fraction': 0.44023691580901386, 'bagging_freq': 4, 'min_child_samples': 64}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 00:59:59,619] Trial 72 finished with value: 0.578579858379229 and parameters: {'num_leaves': 22, 'lambda_l1': 9.039652547294568e-05, 'lambda_l2': 2.6271487603040605e-07, 'feature_fraction': 0.9840015128039508, 'bagging_fraction': 0.4794082120292147, 'bagging_freq': 5, 'min_child_samples': 88}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050728 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:00:16,075] Trial 73 finished with value: 0.6288818006740491 and parameters: {'num_leaves': 68, 'lambda_l1': 0.00017594698429311483, 'lambda_l2': 2.2523560210353525e-06, 'feature_fraction': 0.9438471805648904, 'bagging_fraction': 0.44025891297210257, 'bagging_freq': 4, 'min_child_samples': 67}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:00:28,365] Trial 74 finished with value: 0.6590562138168324 and parameters: {'num_leaves': 43, 'lambda_l1': 0.0012981199877761368, 'lambda_l2': 4.5314029519082213e-07, 'feature_fraction': 0.8785395389848618, 'bagging_fraction': 0.45607297477457515, 'bagging_freq': 7, 'min_child_samples': 55}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:00:44,782] Trial 75 finished with value: 0.6503406942470966 and parameters: {'num_leaves': 74, 'lambda_l1': 0.00042037510896839655, 'lambda_l2': 5.376035572912483e-07, 'feature_fraction': 0.9106549424024075, 'bagging_fraction': 0.4007506779590671, 'bagging_freq': 7, 'min_child_samples': 56}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067862 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:00:59,039] Trial 76 finished with value: 0.6083168441819228 and parameters: {'num_leaves': 53, 'lambda_l1': 0.00023613343770431818, 'lambda_l2': 1.0323801609414317e-06, 'feature_fraction': 0.8816517615832586, 'bagging_fraction': 0.45299239355403326, 'bagging_freq': 6, 'min_child_samples': 46}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:01:13,646] Trial 77 finished with value: 0.6156444860669117 and parameters: {'num_leaves': 62, 'lambda_l1': 0.0012001565973142202, 'lambda_l2': 1.5735367111557758e-07, 'feature_fraction': 0.86904538720203, 'bagging_fraction': 0.48769345297850764, 'bagging_freq': 7, 'min_child_samples': 59}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:01:33,460] Trial 78 finished with value: 0.6364707246615076 and parameters: {'num_leaves': 97, 'lambda_l1': 0.0009431030485272104, 'lambda_l2': 2.8955036083387044e-06, 'feature_fraction': 0.9642916420382845, 'bagging_fraction': 0.4180697869604706, 'bagging_freq': 7, 'min_child_samples': 51}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:01:46,439] Trial 79 finished with value: 0.5653829103890461 and parameters: {'num_leaves': 43, 'lambda_l1': 8.958091814369866e-05, 'lambda_l2': 1.4611205931984816e-06, 'feature_fraction': 0.9296977514555262, 'bagging_fraction': 0.49752949437803456, 'bagging_freq': 4, 'min_child_samples': 94}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:02:05,033] Trial 80 finished with value: 0.6176774956844416 and parameters: {'num_leaves': 87, 'lambda_l1': 0.0075649217343657085, 'lambda_l2': 5.166136262220522e-06, 'feature_fraction': 0.8996609558843044, 'bagging_fraction': 0.4594677113454638, 'bagging_freq': 7, 'min_child_samples': 75}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:02:16,187] Trial 81 finished with value: 0.5976013422267106 and parameters: {'num_leaves': 35, 'lambda_l1': 0.0036735657196867574, 'lambda_l2': 3.6396588899698293e-07, 'feature_fraction': 0.9455679193151991, 'bagging_fraction': 0.43001666447701853, 'bagging_freq': 6, 'min_child_samples': 61}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:02:26,073] Trial 82 finished with value: 0.6216642495625726 and parameters: {'num_leaves': 19, 'lambda_l1': 0.0025129482638147154, 'lambda_l2': 7.056596576088216e-07, 'feature_fraction': 0.9892191937953543, 'bagging_fraction': 0.41441021980270987, 'bagging_freq': 3, 'min_child_samples': 55}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:02:36,903] Trial 83 finished with value: 0.5778884883215707 and parameters: {'num_leaves': 28, 'lambda_l1': 0.0004793815334327517, 'lambda_l2': 1.0581743446358778e-07, 'feature_fraction': 0.9655777659568887, 'bagging_fraction': 0.4379213599718634, 'bagging_freq': 7, 'min_child_samples': 66}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046589 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:02:50,200] Trial 84 finished with value: 0.646976320208556 and parameters: {'num_leaves': 49, 'lambda_l1': 0.0017959761422791387, 'lambda_l2': 2.391298468311191e-07, 'feature_fraction': 0.9269737400094952, 'bagging_fraction': 0.4733481224814492, 'bagging_freq': 4, 'min_child_samples': 69}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:03:03,375] Trial 85 finished with value: 0.6127960707868996 and parameters: {'num_leaves': 38, 'lambda_l1': 0.012916082906252183, 'lambda_l2': 3.6500843698725164e-08, 'feature_fraction': 0.9119948970385325, 'bagging_fraction': 0.417786402086172, 'bagging_freq': 7, 'min_child_samples': 52}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:03:11,396] Trial 86 finished with value: 0.6707434503329145 and parameters: {'num_leaves': 9, 'lambda_l1': 0.004217712154208825, 'lambda_l2': 6.432838085927654e-08, 'feature_fraction': 0.9527305349451941, 'bagging_fraction': 0.45605708541977696, 'bagging_freq': 6, 'min_child_samples': 61}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049758 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:03:18,175] Trial 87 finished with value: 0.6583237432037294 and parameters: {'num_leaves': 6, 'lambda_l1': 0.0008359399025377594, 'lambda_l2': 6.920430527768753e-08, 'feature_fraction': 0.9816220451934015, 'bagging_fraction': 0.4550528339686828, 'bagging_freq': 6, 'min_child_samples': 47}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Melissa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-10-17 01:03:27,088] Trial 88 finished with value: 0.6474731965663422 and parameters: {'num_leaves': 3, 'lambda_l1': 0.004591030400664294, 'lambda_l2': 6.623408868094442e-08, 'feature_fraction': 0.9973281117786053, 'bagging_fraction': 0.452259444213261, 'bagging_freq': 6, 'min_child_samples': 37}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:03:34,659] Trial 89 finished with value: 0.6222169052456051 and parameters: {'num_leaves': 9, 'lambda_l1': 0.0009796595434126596, 'lambda_l2': 1.4371672284889942e-07, 'feature_fraction': 0.9720287632120078, 'bagging_fraction': 0.47348458194366183, 'bagging_freq': 6, 'min_child_samples': 32}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:04:03,714] Trial 90 finished with value: 0.6404802306328311 and parameters: {'num_leaves': 196, 'lambda_l1': 0.0017312864412231664, 'lambda_l2': 4.714320443988561e-08, 'feature_fraction': 0.9848199787895798, 'bagging_fraction': 0.5120840024750115, 'bagging_freq': 6, 'min_child_samples': 42}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:04:12,860] Trial 91 finished with value: 0.5879544547130595 and parameters: {'num_leaves': 18, 'lambda_l1': 0.00017734135877719369, 'lambda_l2': 3.6756586665901883e-07, 'feature_fraction': 0.9596000134349251, 'bagging_fraction': 0.4392064639031666, 'bagging_freq': 5, 'min_child_samples': 60}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057119 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:04:25,057] Trial 92 finished with value: 0.6199989431285743 and parameters: {'num_leaves': 29, 'lambda_l1': 0.00037472702472113355, 'lambda_l2': 1.636988024203969e-08, 'feature_fraction': 0.9335689916132064, 'bagging_fraction': 0.40228994431596005, 'bagging_freq': 4, 'min_child_samples': 48}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Melissa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-10-17 01:04:31,728] Trial 93 finished with value: 0.6614092206160387 and parameters: {'num_leaves': 2, 'lambda_l1': 0.0008793015998396934, 'lambda_l2': 1.9349479101885406e-07, 'feature_fraction': 0.951190197077654, 'bagging_fraction': 0.46368328152187455, 'bagging_freq': 6, 'min_child_samples': 72}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062125 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:04:39,715] Trial 94 finished with value: 0.6144694505442888 and parameters: {'num_leaves': 10, 'lambda_l1': 0.0009513472658254357, 'lambda_l2': 1.0581427991466709e-07, 'feature_fraction': 0.9696970911718185, 'bagging_fraction': 0.4571440385798811, 'bagging_freq': 6, 'min_child_samples': 72}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047761 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:04:49,555] Trial 95 finished with value: 0.5937811923858285 and parameters: {'num_leaves': 22, 'lambda_l1': 0.0005962390269920557, 'lambda_l2': 1.658353881830194e-07, 'feature_fraction': 0.920427635105949, 'bagging_fraction': 0.4915075611378177, 'bagging_freq': 6, 'min_child_samples': 56}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:04:56,314] Trial 96 finished with value: 0.6220480993928861 and parameters: {'num_leaves': 6, 'lambda_l1': 0.004262139272961862, 'lambda_l2': 7.716755125238767e-08, 'feature_fraction': 0.8891161128215702, 'bagging_fraction': 0.46866523798685444, 'bagging_freq': 7, 'min_child_samples': 53}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Melissa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-10-17 01:05:02,235] Trial 97 finished with value: 0.662800767993236 and parameters: {'num_leaves': 3, 'lambda_l1': 0.00013627357111063634, 'lambda_l2': 2.4019592466208413e-07, 'feature_fraction': 0.9859578331095391, 'bagging_fraction': 0.4137522331726591, 'bagging_freq': 6, 'min_child_samples': 77}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Melissa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-10-17 01:05:09,174] Trial 98 finished with value: 0.6563171553718427 and parameters: {'num_leaves': 2, 'lambda_l1': 0.0001118391528574933, 'lambda_l2': 2.2185114379372343e-07, 'feature_fraction': 0.98516905239328, 'bagging_fraction': 0.440342739840473, 'bagging_freq': 6, 'min_child_samples': 78}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042609 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5068\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-17 01:05:32,470] Trial 99 finished with value: 0.6009686520192116 and parameters: {'num_leaves': 141, 'lambda_l1': 0.001719543409270541, 'lambda_l2': 4.054102789329166e-07, 'feature_fraction': 0.9517698054164879, 'bagging_fraction': 0.4607323836036655, 'bagging_freq': 6, 'min_child_samples': 73}. Best is trial 61 with value: 0.6775221062273213.\n"
     ]
    }
   ],
   "source": [
    "optimisation_metric = \"auc\" #\"accuracy\" \"recall\" \"precision\" \"f1\" \"auc\"\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'objective': 'binary',\n",
    "        'metric': 'precision', #\"accuracy\" \"recall\" \"precision\" \"f1\" \"auc\"\n",
    "        'boosting_type': 'gbdt',\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "    \n",
    "    # full data\n",
    "    gbm = lgb.train(param, lgb.Dataset(X_train, y_train), num_boost_round=500)\n",
    "    \n",
    "    preds = gbm.predict(X_val)\n",
    "    y_pred_binary = np.round(preds)\n",
    "    \n",
    "    auc = roc_auc_score(y_val, preds)\n",
    "    accuracy = accuracy_score(y_val, y_pred_binary)\n",
    "    recall = recall_score(y_val, y_pred_binary)\n",
    "    precision = precision_score(y_val, y_pred_binary)\n",
    "    f1 = f1_score(y_val, y_pred_binary)\n",
    "    #choose the metric you want to optimized\n",
    "    \n",
    "    match optimisation_metric:\n",
    "        case \"auc\":\n",
    "            return auc\n",
    "        case \"recall\":\n",
    "            return recall\n",
    "        case \"precision\":\n",
    "            return precision\n",
    "        case \"f1\":\n",
    "            return f1\n",
    "        case _:\n",
    "            return auc\n",
    "    return auc\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Melissa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4330, number of negative: 559772\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035568 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5081\n",
      "[LightGBM] [Info] Number of data points in the train set: 564102, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007676 -> initscore=-4.861962\n",
      "[LightGBM] [Info] Start training from score -4.861962\n"
     ]
    }
   ],
   "source": [
    "best_params['objective'] = 'binary'\n",
    "best_params['metric'] = 'auc' #\"accuracy\" \"recall\" \"precision\" \"f1\" \"auc\"\n",
    "best_params['num_boost_round'] = 500\n",
    "\n",
    "\n",
    "gbm = lgb.train(best_params, lgb.Dataset(X_train_scaled, y_train), num_boost_round=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(X_test)\n",
    "y_pred_binary = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9859614105661384\n",
      "0.853315673848335\n",
      "0.9005305026447933\n",
      "0.6655773382210621\n",
      "0.6661219603583606\n",
      "0.9927532872598344\n",
      "0.5718835835162356\n",
      "0.7671947084413563\n",
      "0.5985441135232504\n",
      "0.9986396654222602\n",
      "0.9847260677472983\n",
      "0.9847260677472983\n",
      "0.9053912791077047\n",
      "0.8664800424880402\n",
      "0.6149118815059738\n",
      "0.5527890387470105\n",
      "0.8661772196553814\n",
      "0.9846477984372463\n",
      "0.6258186573496543\n",
      "0.9448081696224029\n",
      "0.5760756175758571\n",
      "0.758999682860306\n",
      "0.8728082844042839\n",
      "0.9755320136122735\n",
      "0.9846477984372463\n",
      "0.5356823438732697\n",
      "0.7497580975479636\n",
      "0.9306153364384838\n",
      "0.5718835835162356\n",
      "0.9580763568264696\n",
      "0.5718835835162356\n",
      "0.9488964763678299\n",
      "0.78917252943417\n",
      "0.6524661152530559\n",
      "0.827074669247045\n",
      "0.7553554969882975\n",
      "0.8525584457919788\n",
      "0.7867053724465456\n",
      "0.8262219320178741\n",
      "0.8664800424880402\n",
      "0.758999682860306\n",
      "0.6064458788863022\n",
      "0.9759869457162625\n",
      "0.8063722857271868\n",
      "0.5508599395321008\n",
      "0.5485688105646499\n",
      "0.9369819056131319\n",
      "0.9624495773591969\n",
      "0.8804933865578926\n",
      "0.9748036583199928\n",
      "0.8963649778772644\n",
      "0.8743968699691264\n",
      "0.758999682860306\n",
      "0.8063722857271868\n",
      "0.5786049571431862\n",
      "0.9755320136122735\n",
      "0.5913836235735208\n",
      "0.518398691919065\n",
      "0.6974975514899442\n",
      "0.7867053724465456\n",
      "0.9624495773591969\n",
      "0.9053996208250503\n",
      "0.8759625333580106\n",
      "0.827074669247045\n",
      "0.8963649778772644\n",
      "0.7144148196100546\n",
      "0.6974975514899442\n",
      "0.9488964763678299\n",
      "0.7342191805337489\n",
      "0.9846477984372463\n",
      "0.716677435426253\n",
      "0.5333278544509066\n",
      "0.749845144892916\n",
      "0.9005305026447933\n",
      "0.7795078116040273\n",
      "0.7380852188831798\n",
      "0.9053912791077047\n",
      "0.7717450236672436\n",
      "0.7671947084413563\n",
      "0.6978115419788264\n",
      "0.8778503474253465\n",
      "0.982793143930547\n",
      "0.8804933865578926\n",
      "0.9822908270318943\n",
      "0.9847260677472983\n",
      "0.587394287882709\n",
      "0.8041083584669461\n",
      "0.8963649778772644\n",
      "0.8804933865578926\n",
      "0.8778503474253465\n",
      "0.9369819056131319\n",
      "0.8210878766550671\n",
      "0.8788697567257429\n",
      "0.7380852188831798\n",
      "0.758180204025156\n",
      "0.7867053724465456\n",
      "0.6655773382210621\n",
      "0.6968926077056727\n",
      "0.5831928734224373\n",
      "0.8041083584669461\n",
      "0.6845480708081729\n",
      "0.9369819056131319\n",
      "0.5284252988077487\n",
      "0.7717450236672436\n",
      "0.86143825520488\n",
      "0.9193311037095688\n",
      "0.7795078116040273\n",
      "0.9053996208250503\n",
      "0.8003779727813986\n",
      "0.9365205127068137\n",
      "0.7973514003955283\n",
      "0.9624495773591969\n",
      "0.8003779727813986\n",
      "0.5913836235735208\n",
      "0.7786617274137544\n",
      "0.9005305026447933\n",
      "0.5613018834857115\n",
      "0.6156289640409341\n",
      "0.6354369308546488\n",
      "0.7144148196100546\n",
      "0.9560615138042002\n",
      "0.8743968699691264\n",
      "0.7533356977024911\n",
      "0.9887621536251933\n",
      "0.9667754260660005\n",
      "0.9986396654222602\n",
      "0.518398691919065\n",
      "0.5331586263514015\n",
      "0.587394287882709\n",
      "0.758999682860306\n",
      "0.5913836235735208\n",
      "0.7342191805337489\n",
      "0.7671947084413563\n",
      "0.758180204025156\n",
      "0.86143825520488\n",
      "0.9448081696224029\n",
      "0.9560615138042002\n",
      "0.9755320136122735\n",
      "0.8759625333580106\n",
      "0.587394287882709\n",
      "0.8648448464977033\n",
      "0.8062532578194759\n",
      "0.5331586263514015\n",
      "0.8743968699691264\n",
      "0.8210878766550671\n",
      "0.8041083584669461\n",
      "0.9927532872598344\n",
      "0.5718835835162356\n",
      "0.827074669247045\n",
      "0.7533356977024911\n",
      "0.7144148196100546\n",
      "0.685339359904301\n",
      "0.7969455522999742\n",
      "0.6968926077056727\n",
      "0.7355640976447797\n",
      "0.9448081696224029\n",
      "0.685339359904301\n",
      "0.8228578715091422\n",
      "0.758999682860306\n",
      "0.8262219320178741\n",
      "0.9986396654222602\n",
      "0.8228578715091422\n",
      "0.9927532872598344\n",
      "0.6258186573496543\n",
      "0.518398691919065\n",
      "0.6161785273744295\n",
      "0.8664800424880402\n",
      "0.827074669247045\n",
      "0.7969455522999742\n",
      "0.9053912791077047\n",
      "0.8743968699691264\n",
      "0.9560615138042002\n",
      "0.685339359904301\n",
      "0.982793143930547\n",
      "0.827074669247045\n",
      "0.9900664420636133\n",
      "0.7380852188831798\n",
      "0.716677435426253\n",
      "0.7984446965660051\n",
      "0.7717450236672436\n",
      "0.9532065203045246\n",
      "0.6845480708081729\n",
      "0.827074669247045\n",
      "0.7342191805337489\n",
      "0.8228578715091422\n",
      "0.5786049571431862\n",
      "0.8387267318569298\n",
      "0.7671947084413563\n",
      "0.7973514003955283\n",
      "0.8689328569540768\n",
      "0.5662240591225758\n",
      "0.7818477184172401\n",
      "0.86143825520488\n",
      "0.9053996208250503\n",
      "0.5760756175758571\n",
      "0.8759625333580106\n",
      "0.5249521929199126\n",
      "0.5333278544509066\n",
      "0.7818477184172401\n",
      "0.9193311037095688\n",
      "0.915229625436164\n",
      "0.6655773382210621\n",
      "0.8778503474253465\n",
      "0.9759869457162625\n",
      "0.9878497645109493\n",
      "0.7533356977024911\n",
      "0.716677435426253\n",
      "0.6845480708081729\n",
      "0.9488964763678299\n",
      "0.6132362748809582\n",
      "0.9759869457162625\n",
      "0.9667754260660005\n",
      "0.7717450236672436\n",
      "0.6968926077056727\n",
      "0.8062532578194759\n",
      "0.7717450236672436\n",
      "0.8062532578194759\n",
      "0.7867053724465456\n",
      "0.5760756175758571\n",
      "0.7973514003955283\n",
      "0.9297392136491013\n",
      "0.7078810813892005\n",
      "0.7818477184172401\n",
      "0.8210878766550671\n",
      "0.5333278544509066\n",
      "0.758180204025156\n",
      "0.6974975514899442\n",
      "0.7342191805337489\n",
      "0.8262219320178741\n",
      "0.8003779727813986\n",
      "0.982793143930547\n",
      "0.9488964763678299\n",
      "0.915229625436164\n",
      "0.5760756175758571\n",
      "0.758999682860306\n",
      "0.6851765113145943\n",
      "0.9667754260660005\n",
      "0.7717450236672436\n",
      "0.6325936406890297\n",
      "0.8713033931757481\n",
      "0.5485385672276073\n",
      "0.8713033931757481\n",
      "0.5925411767051374\n",
      "0.6256094349423011\n",
      "0.5485385672276073\n",
      "0.5485385672276073\n",
      "0.5034847939975229\n",
      "0.5034847939975229\n",
      "0.5034847939975229\n",
      "0.5109756226676477\n",
      "0.8713033931757481\n",
      "0.6060136161339421\n",
      "0.5957322923938506\n",
      "0.5957322923938506\n",
      "0.5957322923938506\n",
      "0.5702224808357231\n"
     ]
    }
   ],
   "source": [
    "for pred in y_pred:\n",
    "    if pred > 0.5:\n",
    "        print(str(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9554433387791474\n",
      "Recall: 0.16666666666666666\n",
      "Precision: 0.04296875\n",
      "F1 Score: 0.06832298136645963\n",
      "AUC: 0.6212780270077405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x202c9a6a830>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred_binary)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Recall:', recall)\n",
    "print('Precision:', precision)\n",
    "print('F1 Score:', f1)\n",
    "print('AUC:', auc)\n",
    "\n",
    "ConfusionMatrixDisplay(confusion).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = ['Accuracy', 'Recall', 'Precision', 'F1 Score', 'AUC']\n",
    "values = [accuracy, recall, precision, f1, auc]\n",
    "colors = sns.color_palette('pastel', len(metrics))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(metrics, values, color=colors)\n",
    "plt.xlabel('Score')\n",
    "plt.title('Model Performance')\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     feature  importance\n",
      "1            DTDmedianNonFin        5268\n",
      "19         price_to_earnings        2736\n",
      "3                        sic        2503\n",
      "20           retention_ratio        2122\n",
      "7                 cash_ratio        1906\n",
      "13    cashflow_to_debt_ratio        1628\n",
      "14         net_profit_margin        1627\n",
      "18  working_capital_turnover        1473\n",
      "4                        atq        1451\n",
      "15            asset_turnover        1420\n",
      "8        net_working_capital        1395\n",
      "16      receivables_turnover        1313\n",
      "5              current_ratio        1308\n",
      "6                quick_ratio        1305\n",
      "10      debt_to_equity_ratio        1260\n",
      "17     day_sales_outstanding        1218\n",
      "12  financial_leverage_ratio        1204\n",
      "11              equity_ratio        1143\n",
      "9                 debt_ratio        1058\n",
      "0               DTDmedianFin         662\n",
      "2                dummy297fin           0\n"
     ]
    }
   ],
   "source": [
    "importance = pd.DataFrame({'feature': X_train.columns, 'importance': gbm.feature_importance()})\n",
    "importance = importance.sort_values('importance', ascending=False)\n",
    "print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = gbm.feature_importance()\n",
    "features = X_train.columns\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importance})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.sort_values(by='Importance', ascending=False))\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048622 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5081\n",
      "[LightGBM] [Info] Number of data points in the train set: 463380, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score -0.007880\n",
      "[LightGBM] [Info] Start training from score -4.850693\n",
      "[LightGBM] [Info] Start training from score -10.561396\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "model = LGBMClassifier()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "pred = model.predict(X_test_scaled)\n",
    "y_pred_binary = np.round(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.9921\n",
      "Testing accuracy 0.9919\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy {:.4f}'.format(model.score(X_train_scaled,y_train)))\n",
    "print('Testing accuracy {:.4f}'.format(model.score(X_test_scaled,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00    115004\n",
      "         1.0       0.28      0.07      0.11       839\n",
      "         2.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.99    115845\n",
      "   macro avg       0.42      0.36      0.37    115845\n",
      "weighted avg       0.99      0.99      0.99    115845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,model.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Testing LightGBM model with filled_table</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "import pandas_gbq as gbq\n",
    "credentials_path = 'data_cleaning/token.json'\n",
    "\n",
    "# Authenticate with your credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    credentials_path, scopes=['https://www.googleapis.com/auth/bigquery'])\n",
    "\n",
    "# Set the credentials for pandas_gbq\n",
    "gbq.context.credentials = credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'capstone-398012'\n",
    "dataset_id = 'capstone'\n",
    "table_id = \"filled_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/google/cloud/bigquery/table.py:2155: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompNo</th>\n",
       "      <th>yyyy</th>\n",
       "      <th>mm</th>\n",
       "      <th>StkIndx</th>\n",
       "      <th>STInt</th>\n",
       "      <th>dtdlevel</th>\n",
       "      <th>dtdtrend</th>\n",
       "      <th>liqnonfinlevel</th>\n",
       "      <th>liqnonfintrend</th>\n",
       "      <th>ni2talevel</th>\n",
       "      <th>...</th>\n",
       "      <th>nan_count</th>\n",
       "      <th>liqnonfinlevel_notNA</th>\n",
       "      <th>liqnonfintrend_notNA</th>\n",
       "      <th>dtdlevel_notNA</th>\n",
       "      <th>dtdtrend_notNA</th>\n",
       "      <th>DTDmedianNonFin_notNA</th>\n",
       "      <th>Sector_Number_notNA</th>\n",
       "      <th>DTDmedianFin_notNA</th>\n",
       "      <th>sigma_notNA</th>\n",
       "      <th>StkIndx_notNA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26995</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>3.176331</td>\n",
       "      <td>-0.148469</td>\n",
       "      <td>0.280325</td>\n",
       "      <td>-0.146216</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26996</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26996</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27000</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27000</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12335</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12337</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12338</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12339 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CompNo  yyyy mm   StkIndx     STInt  dtdlevel  dtdtrend  \\\n",
       "0       26995  1990  1  0.106263  0.020305  3.176331 -0.148469   \n",
       "1       26996  1990  1  0.106263  0.020305  4.401022  0.054801   \n",
       "2       26996  1990  1  0.106263  0.020305  4.401022  0.054801   \n",
       "3       27000  1990  1  0.106263  0.020305  2.908823  0.056226   \n",
       "4       27000  1990  1  0.106263  0.020305  2.908823  0.056226   \n",
       "...       ...   ... ..       ...       ...       ...       ...   \n",
       "12334   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12335   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12336   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12337   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12338   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "\n",
       "       liqnonfinlevel  liqnonfintrend  ni2talevel  ...  nan_count  \\\n",
       "0            0.280325       -0.146216    0.002395  ...          3   \n",
       "1            0.951410        0.033574    0.002635  ...          3   \n",
       "2            0.951410        0.033574    0.002635  ...          3   \n",
       "3           -0.312616       -0.149733    0.004073  ...          3   \n",
       "4           -0.312616       -0.149733    0.004073  ...          3   \n",
       "...               ...             ...         ...  ...        ...   \n",
       "12334        0.673112       -0.089241    0.006030  ...          0   \n",
       "12335        0.673112       -0.089241    0.006030  ...          0   \n",
       "12336        0.673112       -0.089241    0.006030  ...          0   \n",
       "12337        0.673112       -0.089241    0.006030  ...          0   \n",
       "12338        0.673112       -0.089241    0.006030  ...          0   \n",
       "\n",
       "       liqnonfinlevel_notNA  liqnonfintrend_notNA  dtdlevel_notNA  \\\n",
       "0                         1                     1               0   \n",
       "1                         1                     1               0   \n",
       "2                         1                     1               0   \n",
       "3                         1                     1               0   \n",
       "4                         1                     1               0   \n",
       "...                     ...                   ...             ...   \n",
       "12334                     1                     1               1   \n",
       "12335                     1                     1               1   \n",
       "12336                     1                     1               1   \n",
       "12337                     1                     1               1   \n",
       "12338                     1                     1               1   \n",
       "\n",
       "       dtdtrend_notNA  DTDmedianNonFin_notNA  Sector_Number_notNA  \\\n",
       "0                   0                      0                    1   \n",
       "1                   0                      0                    1   \n",
       "2                   0                      0                    1   \n",
       "3                   0                      0                    1   \n",
       "4                   0                      0                    1   \n",
       "...               ...                    ...                  ...   \n",
       "12334               1                      1                    1   \n",
       "12335               1                      1                    1   \n",
       "12336               1                      1                    1   \n",
       "12337               1                      1                    1   \n",
       "12338               1                      1                    1   \n",
       "\n",
       "       DTDmedianFin_notNA  sigma_notNA  StkIndx_notNA  \n",
       "0                       1            1              1  \n",
       "1                       1            1              1  \n",
       "2                       1            1              1  \n",
       "3                       1            1              1  \n",
       "4                       1            1              1  \n",
       "...                   ...          ...            ...  \n",
       "12334                   1            1              1  \n",
       "12335                   1            1              1  \n",
       "12336                   1            1              1  \n",
       "12337                   1            1              1  \n",
       "12338                   1            1              1  \n",
       "\n",
       "[12339 rows x 34 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas_gbq import read_gbq\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{project_id}.{dataset_id}.{table_id}`\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authenticate and read data from BigQuery into a DataFrame\n",
    "df = read_gbq(query, project_id=project_id, dialect='standard')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompNo</th>\n",
       "      <th>yyyy</th>\n",
       "      <th>mm</th>\n",
       "      <th>StkIndx</th>\n",
       "      <th>STInt</th>\n",
       "      <th>dtdlevel</th>\n",
       "      <th>dtdtrend</th>\n",
       "      <th>liqnonfinlevel</th>\n",
       "      <th>liqnonfintrend</th>\n",
       "      <th>ni2talevel</th>\n",
       "      <th>...</th>\n",
       "      <th>nan_count</th>\n",
       "      <th>liqnonfinlevel_notNA</th>\n",
       "      <th>liqnonfintrend_notNA</th>\n",
       "      <th>dtdlevel_notNA</th>\n",
       "      <th>dtdtrend_notNA</th>\n",
       "      <th>DTDmedianNonFin_notNA</th>\n",
       "      <th>Sector_Number_notNA</th>\n",
       "      <th>DTDmedianFin_notNA</th>\n",
       "      <th>sigma_notNA</th>\n",
       "      <th>StkIndx_notNA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26995</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>3.176331</td>\n",
       "      <td>-0.148469</td>\n",
       "      <td>0.280325</td>\n",
       "      <td>-0.146216</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26996</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26996</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27000</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27000</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12335</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12337</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12338</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12178 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CompNo  yyyy mm   StkIndx     STInt  dtdlevel  dtdtrend  \\\n",
       "0       26995  1990  1  0.106263  0.020305  3.176331 -0.148469   \n",
       "1       26996  1990  1  0.106263  0.020305  4.401022  0.054801   \n",
       "2       26996  1990  1  0.106263  0.020305  4.401022  0.054801   \n",
       "3       27000  1990  1  0.106263  0.020305  2.908823  0.056226   \n",
       "4       27000  1990  1  0.106263  0.020305  2.908823  0.056226   \n",
       "...       ...   ... ..       ...       ...       ...       ...   \n",
       "12334   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12335   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12336   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12337   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12338   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "\n",
       "       liqnonfinlevel  liqnonfintrend  ni2talevel  ...  nan_count  \\\n",
       "0            0.280325       -0.146216    0.002395  ...          3   \n",
       "1            0.951410        0.033574    0.002635  ...          3   \n",
       "2            0.951410        0.033574    0.002635  ...          3   \n",
       "3           -0.312616       -0.149733    0.004073  ...          3   \n",
       "4           -0.312616       -0.149733    0.004073  ...          3   \n",
       "...               ...             ...         ...  ...        ...   \n",
       "12334        0.673112       -0.089241    0.006030  ...          0   \n",
       "12335        0.673112       -0.089241    0.006030  ...          0   \n",
       "12336        0.673112       -0.089241    0.006030  ...          0   \n",
       "12337        0.673112       -0.089241    0.006030  ...          0   \n",
       "12338        0.673112       -0.089241    0.006030  ...          0   \n",
       "\n",
       "       liqnonfinlevel_notNA  liqnonfintrend_notNA  dtdlevel_notNA  \\\n",
       "0                         1                     1               0   \n",
       "1                         1                     1               0   \n",
       "2                         1                     1               0   \n",
       "3                         1                     1               0   \n",
       "4                         1                     1               0   \n",
       "...                     ...                   ...             ...   \n",
       "12334                     1                     1               1   \n",
       "12335                     1                     1               1   \n",
       "12336                     1                     1               1   \n",
       "12337                     1                     1               1   \n",
       "12338                     1                     1               1   \n",
       "\n",
       "       dtdtrend_notNA  DTDmedianNonFin_notNA  Sector_Number_notNA  \\\n",
       "0                   0                      0                    1   \n",
       "1                   0                      0                    1   \n",
       "2                   0                      0                    1   \n",
       "3                   0                      0                    1   \n",
       "4                   0                      0                    1   \n",
       "...               ...                    ...                  ...   \n",
       "12334               1                      1                    1   \n",
       "12335               1                      1                    1   \n",
       "12336               1                      1                    1   \n",
       "12337               1                      1                    1   \n",
       "12338               1                      1                    1   \n",
       "\n",
       "       DTDmedianFin_notNA  sigma_notNA  StkIndx_notNA  \n",
       "0                       1            1              1  \n",
       "1                       1            1              1  \n",
       "2                       1            1              1  \n",
       "3                       1            1              1  \n",
       "4                       1            1              1  \n",
       "...                   ...          ...            ...  \n",
       "12334                   1            1              1  \n",
       "12335                   1            1              1  \n",
       "12336                   1            1              1  \n",
       "12337                   1            1              1  \n",
       "12338                   1            1              1  \n",
       "\n",
       "[12178 rows x 34 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop na rows\n",
    "test_df = df.dropna()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rz/d40gjr2505ddk3wl3mnzmvdm0000gn/T/ipykernel_15768/3448417843.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['Event_type'] = test_df['Event_type'].map(lambda x : 0 if x ==2  else x)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompNo</th>\n",
       "      <th>yyyy</th>\n",
       "      <th>mm</th>\n",
       "      <th>StkIndx</th>\n",
       "      <th>STInt</th>\n",
       "      <th>dtdlevel</th>\n",
       "      <th>dtdtrend</th>\n",
       "      <th>liqnonfinlevel</th>\n",
       "      <th>liqnonfintrend</th>\n",
       "      <th>ni2talevel</th>\n",
       "      <th>...</th>\n",
       "      <th>nan_count</th>\n",
       "      <th>liqnonfinlevel_notNA</th>\n",
       "      <th>liqnonfintrend_notNA</th>\n",
       "      <th>dtdlevel_notNA</th>\n",
       "      <th>dtdtrend_notNA</th>\n",
       "      <th>DTDmedianNonFin_notNA</th>\n",
       "      <th>Sector_Number_notNA</th>\n",
       "      <th>DTDmedianFin_notNA</th>\n",
       "      <th>sigma_notNA</th>\n",
       "      <th>StkIndx_notNA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26995</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>3.176331</td>\n",
       "      <td>-0.148469</td>\n",
       "      <td>0.280325</td>\n",
       "      <td>-0.146216</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26996</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26996</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27000</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27000</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12335</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12337</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12338</th>\n",
       "      <td>27058</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12178 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CompNo  yyyy mm   StkIndx     STInt  dtdlevel  dtdtrend  \\\n",
       "0       26995  1990  1  0.106263  0.020305  3.176331 -0.148469   \n",
       "1       26996  1990  1  0.106263  0.020305  4.401022  0.054801   \n",
       "2       26996  1990  1  0.106263  0.020305  4.401022  0.054801   \n",
       "3       27000  1990  1  0.106263  0.020305  2.908823  0.056226   \n",
       "4       27000  1990  1  0.106263  0.020305  2.908823  0.056226   \n",
       "...       ...   ... ..       ...       ...       ...       ...   \n",
       "12334   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12335   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12336   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12337   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "12338   27058  2023  7  0.110445  0.009928  6.832332  2.326057   \n",
       "\n",
       "       liqnonfinlevel  liqnonfintrend  ni2talevel  ...  nan_count  \\\n",
       "0            0.280325       -0.146216    0.002395  ...          3   \n",
       "1            0.951410        0.033574    0.002635  ...          3   \n",
       "2            0.951410        0.033574    0.002635  ...          3   \n",
       "3           -0.312616       -0.149733    0.004073  ...          3   \n",
       "4           -0.312616       -0.149733    0.004073  ...          3   \n",
       "...               ...             ...         ...  ...        ...   \n",
       "12334        0.673112       -0.089241    0.006030  ...          0   \n",
       "12335        0.673112       -0.089241    0.006030  ...          0   \n",
       "12336        0.673112       -0.089241    0.006030  ...          0   \n",
       "12337        0.673112       -0.089241    0.006030  ...          0   \n",
       "12338        0.673112       -0.089241    0.006030  ...          0   \n",
       "\n",
       "       liqnonfinlevel_notNA  liqnonfintrend_notNA  dtdlevel_notNA  \\\n",
       "0                         1                     1               0   \n",
       "1                         1                     1               0   \n",
       "2                         1                     1               0   \n",
       "3                         1                     1               0   \n",
       "4                         1                     1               0   \n",
       "...                     ...                   ...             ...   \n",
       "12334                     1                     1               1   \n",
       "12335                     1                     1               1   \n",
       "12336                     1                     1               1   \n",
       "12337                     1                     1               1   \n",
       "12338                     1                     1               1   \n",
       "\n",
       "       dtdtrend_notNA  DTDmedianNonFin_notNA  Sector_Number_notNA  \\\n",
       "0                   0                      0                    1   \n",
       "1                   0                      0                    1   \n",
       "2                   0                      0                    1   \n",
       "3                   0                      0                    1   \n",
       "4                   0                      0                    1   \n",
       "...               ...                    ...                  ...   \n",
       "12334               1                      1                    1   \n",
       "12335               1                      1                    1   \n",
       "12336               1                      1                    1   \n",
       "12337               1                      1                    1   \n",
       "12338               1                      1                    1   \n",
       "\n",
       "       DTDmedianFin_notNA  sigma_notNA  StkIndx_notNA  \n",
       "0                       1            1              1  \n",
       "1                       1            1              1  \n",
       "2                       1            1              1  \n",
       "3                       1            1              1  \n",
       "4                       1            1              1  \n",
       "...                   ...          ...            ...  \n",
       "12334                   1            1              1  \n",
       "12335                   1            1              1  \n",
       "12336                   1            1              1  \n",
       "12337                   1            1              1  \n",
       "12338                   1            1              1  \n",
       "\n",
       "[12178 rows x 34 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map event type\n",
    "test_df['Event_type'] = test_df['Event_type'].map(lambda x : 0 if x ==2  else x)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_df.drop(['Event_type', 'yyyy', 'mm', 'EventDate'], axis=1)  # Features\n",
    "y = test_df['Event_type']  # Target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 727, number of negative: 9015\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4108\n",
      "[LightGBM] [Info] Number of data points in the train set: 9742, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.889678 -> initscore=2.087452\n",
      "[LightGBM] [Info] Start training from score 2.087452\n"
     ]
    }
   ],
   "source": [
    "class_weights = {0:1, 1:100}\n",
    "model = LGBMClassifier( class_weight = class_weights)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.7540\n",
      "Testing accuracy 0.6773\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy {:.4f}'.format(model.score(X_train,y_train)))\n",
    "print('Testing accuracy {:.4f}'.format(model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.68      0.80      2254\n",
      "           1       0.14      0.67      0.24       182\n",
      "\n",
      "    accuracy                           0.68      2436\n",
      "   macro avg       0.55      0.67      0.52      2436\n",
      "weighted avg       0.90      0.68      0.75      2436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "push updated table to gbq, ie the table used for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompNo</th>\n",
       "      <th>StkIndx</th>\n",
       "      <th>STInt</th>\n",
       "      <th>dtdlevel</th>\n",
       "      <th>dtdtrend</th>\n",
       "      <th>liqnonfinlevel</th>\n",
       "      <th>liqnonfintrend</th>\n",
       "      <th>ni2talevel</th>\n",
       "      <th>ni2tatrend</th>\n",
       "      <th>sizelevel</th>\n",
       "      <th>...</th>\n",
       "      <th>nan_count</th>\n",
       "      <th>liqnonfinlevel_notNA</th>\n",
       "      <th>liqnonfintrend_notNA</th>\n",
       "      <th>dtdlevel_notNA</th>\n",
       "      <th>dtdtrend_notNA</th>\n",
       "      <th>DTDmedianNonFin_notNA</th>\n",
       "      <th>Sector_Number_notNA</th>\n",
       "      <th>DTDmedianFin_notNA</th>\n",
       "      <th>sigma_notNA</th>\n",
       "      <th>StkIndx_notNA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26995</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>3.176331</td>\n",
       "      <td>-0.148469</td>\n",
       "      <td>0.280325</td>\n",
       "      <td>-0.146216</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26996</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>-0.000224</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26996</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>4.401022</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0.951410</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>-0.000224</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27000</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>1.248517</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27000</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>2.908823</td>\n",
       "      <td>0.056226</td>\n",
       "      <td>-0.312616</td>\n",
       "      <td>-0.149733</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>1.248517</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>27058</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>5.101596</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12335</th>\n",
       "      <td>27058</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>5.101596</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>27058</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>5.101596</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12337</th>\n",
       "      <td>27058</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>5.101596</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12338</th>\n",
       "      <td>27058</td>\n",
       "      <td>0.110445</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>6.832332</td>\n",
       "      <td>2.326057</td>\n",
       "      <td>0.673112</td>\n",
       "      <td>-0.089241</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>5.101596</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12178 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CompNo   StkIndx     STInt  dtdlevel  dtdtrend  liqnonfinlevel  \\\n",
       "0       26995  0.106263  0.020305  3.176331 -0.148469        0.280325   \n",
       "1       26996  0.106263  0.020305  4.401022  0.054801        0.951410   \n",
       "2       26996  0.106263  0.020305  4.401022  0.054801        0.951410   \n",
       "3       27000  0.106263  0.020305  2.908823  0.056226       -0.312616   \n",
       "4       27000  0.106263  0.020305  2.908823  0.056226       -0.312616   \n",
       "...       ...       ...       ...       ...       ...             ...   \n",
       "12334   27058  0.110445  0.009928  6.832332  2.326057        0.673112   \n",
       "12335   27058  0.110445  0.009928  6.832332  2.326057        0.673112   \n",
       "12336   27058  0.110445  0.009928  6.832332  2.326057        0.673112   \n",
       "12337   27058  0.110445  0.009928  6.832332  2.326057        0.673112   \n",
       "12338   27058  0.110445  0.009928  6.832332  2.326057        0.673112   \n",
       "\n",
       "       liqnonfintrend  ni2talevel  ni2tatrend  sizelevel  ...  nan_count  \\\n",
       "0           -0.146216    0.002395    0.001367   0.666644  ...          3   \n",
       "1            0.033574    0.002635   -0.000224   0.014110  ...          3   \n",
       "2            0.033574    0.002635   -0.000224   0.014110  ...          3   \n",
       "3           -0.149733    0.004073    0.004625   1.248517  ...          3   \n",
       "4           -0.149733    0.004073    0.004625   1.248517  ...          3   \n",
       "...               ...         ...         ...        ...  ...        ...   \n",
       "12334       -0.089241    0.006030    0.000547   5.101596  ...          0   \n",
       "12335       -0.089241    0.006030    0.000547   5.101596  ...          0   \n",
       "12336       -0.089241    0.006030    0.000547   5.101596  ...          0   \n",
       "12337       -0.089241    0.006030    0.000547   5.101596  ...          0   \n",
       "12338       -0.089241    0.006030    0.000547   5.101596  ...          0   \n",
       "\n",
       "       liqnonfinlevel_notNA  liqnonfintrend_notNA  dtdlevel_notNA  \\\n",
       "0                         1                     1               0   \n",
       "1                         1                     1               0   \n",
       "2                         1                     1               0   \n",
       "3                         1                     1               0   \n",
       "4                         1                     1               0   \n",
       "...                     ...                   ...             ...   \n",
       "12334                     1                     1               1   \n",
       "12335                     1                     1               1   \n",
       "12336                     1                     1               1   \n",
       "12337                     1                     1               1   \n",
       "12338                     1                     1               1   \n",
       "\n",
       "       dtdtrend_notNA  DTDmedianNonFin_notNA  Sector_Number_notNA  \\\n",
       "0                   0                      0                    1   \n",
       "1                   0                      0                    1   \n",
       "2                   0                      0                    1   \n",
       "3                   0                      0                    1   \n",
       "4                   0                      0                    1   \n",
       "...               ...                    ...                  ...   \n",
       "12334               1                      1                    1   \n",
       "12335               1                      1                    1   \n",
       "12336               1                      1                    1   \n",
       "12337               1                      1                    1   \n",
       "12338               1                      1                    1   \n",
       "\n",
       "       DTDmedianFin_notNA  sigma_notNA  StkIndx_notNA  \n",
       "0                       1            1              1  \n",
       "1                       1            1              1  \n",
       "2                       1            1              1  \n",
       "3                       1            1              1  \n",
       "4                       1            1              1  \n",
       "...                   ...          ...            ...  \n",
       "12334                   1            1              1  \n",
       "12335                   1            1              1  \n",
       "12336                   1            1              1  \n",
       "12337                   1            1              1  \n",
       "12338                   1            1              1  \n",
       "\n",
       "[12178 rows x 31 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = df.dropna()\n",
    "new_df = new_df.drop(['yyyy', 'mm', 'EventDate'], axis=1,)\n",
    "new_df['Event_type'] = new_df['Event_type'].map(lambda x : 0 if x ==2  else x)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_gbq(destination_table=f'{project_id}.{dataset_id}.filled_table', project_id=project_id, if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
